{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f230295c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66a3588e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2aa4fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregunta</th>\n",
       "      <th>Contexto</th>\n",
       "      <th>Respuesta_Modelo</th>\n",
       "      <th>Respuesta_Referencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>¿Qué es una RNN?</td>\n",
       "      <td>[2.3 Ejemplo RNN\\n\\nEstas redes se suelen util...</td>\n",
       "      <td>Una RNN, o red neuronal recurrente, es un tipo...</td>\n",
       "      <td>Las Redes Neuronales Recurrentes son un tipo e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Pregunta                                           Contexto  \\\n",
       "0  ¿Qué es una RNN?  [2.3 Ejemplo RNN\\n\\nEstas redes se suelen util...   \n",
       "\n",
       "                                    Respuesta_Modelo  \\\n",
       "0  Una RNN, o red neuronal recurrente, es un tipo...   \n",
       "\n",
       "                                Respuesta_Referencia  \n",
       "0  Las Redes Neuronales Recurrentes son un tipo e...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    'Pregunta': [\"¿Qué es una RNN?\"],\n",
    "    'Contexto': [['2.3 Ejemplo RNN\\n\\nEstas redes se suelen utilizar para problemas de modelado de lenguaje. Muestra un ejemplo en el que se utiliza una RNN N a N para predecir la siguiente letra de una palabra (hola) y muestra que al introducir la letra ”h”, la red calcula un estado oculto h1 y deduce la letra ”o”. Luego, al introducir la letra ”o” como input, y usando el vector de estado oculto h1, la red calcula el siguiente estado oculto h2 y deduce la letra ”l”...\\n\\n2.4 Aplicaci´on sobre im´agenes\\n\\nPara su aplicaci´on sobre im´agenes, se utiliza una red convolucional sobre la imagen para extraer las caracter´ısticas de la imagen y luego se utiliza una RNN para procesar estas caracter´ısticas y devolver el texto de salida.\\n\\nEn este caso, la ecuaci´on para calcular el estado oculto ser´ıa:\\n\\nht = tanh(whhht−1 + wxhxt + wihit) (5)\\n\\nDonde wih es la matriz de pesos que multiplicamos por el vector de caracter´ısticas extra´ıdas de la imagen it.\\n\\n3\\n\\nIntroducci´on a LSTM\\n\\n3.1 LSTM\\n\\nLas LSTMs (Long Short-Term Memory) son un tipo de RNN que fueron introducidas para resolver uno de los principales problemas de las RNN est´andar, el desvanecimiento del gradiente. El desvanecimiento del gradiente es un problema que ocurre cuando las redes neuronales tienen problemas para aprender y recordar informaci´on a lo largo de secuencias largas. Esto se debe a la disminuci´on progresiva de los gradientes que se calculan durante el entrenamiento, por lo que tienden a olvidar informaci´on a largo plazo.\\n\\nEstas redes resuelven este problema introduciendo un mecanismo de celdas de memoria, que puede retener informaci´on a lo largo de la secuencia. Esta capacidad para mantener informaci´on durante muchas iteraciones hace que las LSTM sean ideales para tareas que requieran procesamiento de datos temporales, NLP, etc.\\n\\nLas LSTM son una sofisticaci´on de las RNN tradicionales, que introducen el concepto de la celda de memoria. Esta celda de memoria es una estructura que se encarga de mantener la informaci´on relevante durante muchos pasos, evitando que se pierda o degrade la informaci´on conforme avanza la secuencia. Esta c´elula de memoria est´a controlada por una serie de mecanismos, llamados puertas que regulan la informaci´on que entra y sale de la celda de memoria y permiten que la red decida de una forma m´as eficiente qu´e informaci´on mantener y qu´e informaci´on descartar.',\n",
    " '1.1 Problemas de las RNN est´andar\\n\\nEstas redes se enfrentan a un problema fundamental, el Desvanecimiento del gradiente. Estas redes se entrenan con un tipo de backpropagation, conocido como Backpropagation Through Time (BPTT). En este algoritmo, el gradiente se propaga hacia atr´as en el tiempo y se ajustan los pesos de la red para minimizar el error. El problema es que, a medida que el error se propaga hacia atr´as en el tiempo, si las secuencias son muy largas, los gradientes se van desvaneciendo, por lo tanto, el impacto del aprendizaje de los primeros pasos de la secuencia es muy peque˜no. Esto hace que las RNN tengan dificultades para aprender de elementos que est´an muy lejos en la secuencia. Cabe destacar que tambi´en puede ocurrir el efecto contrario, la Explosi´on del gradiente, donde los gradientes crecen exponencialmente a medida que se propagan hacia atr´as en el tiempo, por lo que producen inestabilidad en el entrenamiento de la red.\\n\\nEsta es la principal limitaci´on de las RNNs est´andar, que son eficaces a corto plazo. Pero cuando las secuencias son muy largas o la informaci´on relevante est´a muy separada en la secuencia, su rendimiento disminuye significativamente.\\n\\nEste problema ha impulsado que se investigue y se desarrollen nuevas arquitecturas de RNN, como las LSTM y las GRU, que son capaces de recordar informaci´on a largo plazo y evitar el desvanecimiento del gradiente.\\n\\n2 RNN\\n\\nEstas redes permiten usar una variedad m´as amplia de estructuras tanto de entrada como de salida:\\n\\n• 1 a 1: Redes convencionales, un solo input y un solo output.\\n\\n• 1 a N: Por ejemplo: Generaci´on de texto a partir de una imagen.\\n\\nN a 1: Por ejemplo: An´alisis de sentimientos a partir de una rese˜na (de N palabras).\\n\\nN a N: Por ejemplo: Traducci´on de un idioma a otro. Tiene N palabras de entrada y M palabras\\n\\nde salida ya que la traducci´on puede tener m´as o menos palabras que el texto original por lo que t´ecnicamente es N a M.\\n\\nLa funci´on matem´atica que define una RNN es la siguiente:\\n\\nht = fω(ht−1,xt) (1)\\n\\nDonde:\\n\\nht es el estado oculto en el tiempo t.\\n\\nfω es la funci´on de activaci´on. Depende de los pesos ω de la red.\\n\\nht−1 es el estado oculto en el instante anterior, t − 1.\\n\\nxt es el input en el tiempo t.\\n\\n2',\n",
    " '2.1 Vanilla RNN\\n\\nVanilla RNN es la forma m´as b´asica de RNN. La f´ormula de la RNN est´andar es la siguiente:\\n\\nht = fω(ht−1,xt) (2)\\n\\nht = tanh(whhht−1 + wxhxt) (3)\\n\\nDonde:\\n\\nwhh es la matriz de pesos que multiplicamos por el estado oculto anterior ht−1.\\n\\nwxh es la matriz de pesos que multiplicamos por el input actual xt.\\n\\n• tanh es la funci´on de activaci´on tangente hiperb´olica. Esto se hace para obtener una no linealidad en el sistema.\\n\\nY la salida de la red es:\\n\\nyt = whyht (4)\\n\\nDonde why es la matriz de pesos que multiplicamos por el estado oculto ht para obtener la salida yt.\\n\\n2.2 Flujo de RNN\\n\\nAqu´ı se muestra una imagen con cajas que describe el flujo de los datos en una RNN con cajas. En t = 0, se aplica la funci´on fω a la entrada x0 y al estado oculto h0, que es el estado oculto inicial. Esto genera un nuevo estado oculto h1. Luego, este nuevo estado oculto se utiliza junto con la entrada x1 en la funci´on fω para generar el siguiente estado oculto h2. Este proceso se repite para cada entrada en la secuencia. En este caso, usamos siempre la matriz de pesos w para todas las interacciones, lo que significa que la red tiene los mismos pesos para cada paso de tiempo.\\n\\nPara aplicar el algoritmo de backpropagation, la informaci´on se propaga hacia atr´as a trav´es de la secuencia. Cada instante temporal tiene su gradiente asociado, as´ı que el gradiente para la matriz de pesos w se calcula como la suma de los gradientes de cada instante temporal.\\n\\nPara obtener las salidas para cada instante temporal (yt), se multiplica el estado oculto correspondi- ente ht por la matriz de pesos why.\\n\\nLa p´erdida total del modelo se calcula como la suma de las p´erdidas de cada instante temporal. La funci´on de p´erdida se puede calcular como la suma de las p´erdidas individuales para cada instante temporal, y luego se propaga hacia atr´as a trav´es de la secuencia para actualizar los pesos.\\n\\nEsto se hace para los N a N, si es un N a 1, se hace lo mismo pero solo se calcula la salida para el ´ultimo instante temporal. En el caso de 1 a N, solo se aplica el input al instante temporal 0 y se calcula la salida para todos los instantes temporales.\\n\\nComenta tambi´en que un encoder ser´ıa un modelo N a 1 y un decoder ser´ıa un modelo 1 a N. Cada modelo tiene su matriz w.']],\n",
    "    'Respuesta_Modelo': [\"\"\"Una RNN, o red neuronal recurrente, es un tipo de red neuronal diseñada para procesar secuencias de datos. A diferencia de las redes neuronales convencionales, que tienen una estructura estática, las RNN tienen conexiones que permiten que la información persista. Esto se logra a través de un estado oculto que se actualiza en cada paso de tiempo en función de la entrada actual y del estado oculto anterior. La función matemática que define una RNN es:\n",
    "\n",
    "[ h_t = f_\\omega(h_{t-1}, x_t) ]\n",
    "\n",
    "donde ( h_t ) es el estado oculto en el tiempo ( t ), ( h_{t-1} ) es el estado oculto en el instante anterior, y ( x_t ) es la entrada en el tiempo ( t ).\n",
    "\n",
    "Las RNN son útiles en diversas aplicaciones, como modelado de lenguaje, generación de texto, análisis de sentimientos, y traducción de lenguajes, ya que pueden manejar entradas y salidas de longitud variable. Sin embargo, enfrentan problemas como el desvanecimiento del gradiente, que dificulta el aprendizaje de secuencias largas. Para superar estas limitaciones, se han desarrollado arquitecturas como LSTMs y GRUs que son capaces de recordar información a largo plazo.\n",
    "\n",
    "Esta información se encuentra en la sección 2 RNN (páginas 2-3) y en la sección 3 Introducción a LSTM (páginas 3-4).\"\"\"],\n",
    "    'Respuesta_Referencia': [\"\"\"Las Redes Neuronales Recurrentes son un tipo especifico de arquitectura de red neuronal que esta\n",
    "disenado para manejar datos secuenciales, lo que significa que el orden de los datos es muy importante.\n",
    "Este tipo de red es fundamental en tareas donde la informacion anterior debe ser recordada o\n",
    "procesada junto a la informacion actual. A lo largo de la historia de la IA, las RNN han sido muy importantes\n",
    "a la hora de resolver problemas relacionados con el analisis de frecuencias como NLP, predicciones\n",
    "de series temporales, transcripcion de voz a texto o modelado de video y audio.\n",
    "En las redes convencionales como las feedforward, la salida solo depende del input actual. Sin\n",
    "embargo, en las RNN, la salida depende tanto del input actual como de los inputs anteriores al introducir\n",
    "el concepto de memoria. Esta capacidad de recordar informacion previa es lo que hace que las RNN sean\n",
    "tan poderosas en la resolucion de problemas secuenciales.\"\"\"] \n",
    "})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17879206",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[len(df)] = [\n",
    "    \"¿Qué limitaciones tienen las RNNs?\",\n",
    "    ['1.1 Problemas de las RNN est´andar\\n\\nEstas redes se enfrentan a un problema fundamental, el Desvanecimiento del gradiente. Estas redes se entrenan con un tipo de backpropagation, conocido como Backpropagation Through Time (BPTT). En este algoritmo, el gradiente se propaga hacia atr´as en el tiempo y se ajustan los pesos de la red para minimizar el error. El problema es que, a medida que el error se propaga hacia atr´as en el tiempo, si las secuencias son muy largas, los gradientes se van desvaneciendo, por lo tanto, el impacto del aprendizaje de los primeros pasos de la secuencia es muy peque˜no. Esto hace que las RNN tengan dificultades para aprender de elementos que est´an muy lejos en la secuencia. Cabe destacar que tambi´en puede ocurrir el efecto contrario, la Explosi´on del gradiente, donde los gradientes crecen exponencialmente a medida que se propagan hacia atr´as en el tiempo, por lo que producen inestabilidad en el entrenamiento de la red.\\n\\nEsta es la principal limitaci´on de las RNNs est´andar, que son eficaces a corto plazo. Pero cuando las secuencias son muy largas o la informaci´on relevante est´a muy separada en la secuencia, su rendimiento disminuye significativamente.\\n\\nEste problema ha impulsado que se investigue y se desarrollen nuevas arquitecturas de RNN, como las LSTM y las GRU, que son capaces de recordar informaci´on a largo plazo y evitar el desvanecimiento del gradiente.\\n\\n2 RNN\\n\\nEstas redes permiten usar una variedad m´as amplia de estructuras tanto de entrada como de salida:\\n\\n• 1 a 1: Redes convencionales, un solo input y un solo output.\\n\\n• 1 a N: Por ejemplo: Generaci´on de texto a partir de una imagen.\\n\\nN a 1: Por ejemplo: An´alisis de sentimientos a partir de una rese˜na (de N palabras).\\n\\nN a N: Por ejemplo: Traducci´on de un idioma a otro. Tiene N palabras de entrada y M palabras\\n\\nde salida ya que la traducci´on puede tener m´as o menos palabras que el texto original por lo que t´ecnicamente es N a M.\\n\\nLa funci´on matem´atica que define una RNN es la siguiente:\\n\\nht = fω(ht−1,xt) (1)\\n\\nDonde:\\n\\nht es el estado oculto en el tiempo t.\\n\\nfω es la funci´on de activaci´on. Depende de los pesos ω de la red.\\n\\nht−1 es el estado oculto en el instante anterior, t − 1.\\n\\nxt es el input en el tiempo t.\\n\\n2',\n",
    " '2.3 Ejemplo RNN\\n\\nEstas redes se suelen utilizar para problemas de modelado de lenguaje. Muestra un ejemplo en el que se utiliza una RNN N a N para predecir la siguiente letra de una palabra (hola) y muestra que al introducir la letra ”h”, la red calcula un estado oculto h1 y deduce la letra ”o”. Luego, al introducir la letra ”o” como input, y usando el vector de estado oculto h1, la red calcula el siguiente estado oculto h2 y deduce la letra ”l”...\\n\\n2.4 Aplicaci´on sobre im´agenes\\n\\nPara su aplicaci´on sobre im´agenes, se utiliza una red convolucional sobre la imagen para extraer las caracter´ısticas de la imagen y luego se utiliza una RNN para procesar estas caracter´ısticas y devolver el texto de salida.\\n\\nEn este caso, la ecuaci´on para calcular el estado oculto ser´ıa:\\n\\nht = tanh(whhht−1 + wxhxt + wihit) (5)\\n\\nDonde wih es la matriz de pesos que multiplicamos por el vector de caracter´ısticas extra´ıdas de la imagen it.\\n\\n3\\n\\nIntroducci´on a LSTM\\n\\n3.1 LSTM\\n\\nLas LSTMs (Long Short-Term Memory) son un tipo de RNN que fueron introducidas para resolver uno de los principales problemas de las RNN est´andar, el desvanecimiento del gradiente. El desvanecimiento del gradiente es un problema que ocurre cuando las redes neuronales tienen problemas para aprender y recordar informaci´on a lo largo de secuencias largas. Esto se debe a la disminuci´on progresiva de los gradientes que se calculan durante el entrenamiento, por lo que tienden a olvidar informaci´on a largo plazo.\\n\\nEstas redes resuelven este problema introduciendo un mecanismo de celdas de memoria, que puede retener informaci´on a lo largo de la secuencia. Esta capacidad para mantener informaci´on durante muchas iteraciones hace que las LSTM sean ideales para tareas que requieran procesamiento de datos temporales, NLP, etc.\\n\\nLas LSTM son una sofisticaci´on de las RNN tradicionales, que introducen el concepto de la celda de memoria. Esta celda de memoria es una estructura que se encarga de mantener la informaci´on relevante durante muchos pasos, evitando que se pierda o degrade la informaci´on conforme avanza la secuencia. Esta c´elula de memoria est´a controlada por una serie de mecanismos, llamados puertas que regulan la informaci´on que entra y sale de la celda de memoria y permiten que la red decida de una forma m´as eficiente qu´e informaci´on mantener y qu´e informaci´on descartar.',\n",
    " '6 Limitaciones y mejores pr´acticas de LSTM\\n\\n6.1 Limitaciones de las LSTM\\n\\n• Complejidad y coste computacional: las puertas y la c´elula de memoria a˜naden complejidad a la red, adem´as de aumentar el coste computacional y el tiempo de entrenamiento. As´ı que en casos en los que haya recursos limitados o se necesite un entrenamiento r´apido, las LSTM pueden no ser la mejor opci´on.\\n\\n• Problemas con secuencias extremadamente largas: aunque son bastante mejores que las RNNs en escenarios donde las secuencias son extremadamente largas, pueden seguir teniendo prob- lemas con el desvanecimiento del gradiente a muy largo plazo perdiendo informaci´on relevante.\\n\\n• Riesgo de sobreajuste: este riesgo es considerable en conjuntos de datos muy peque˜nos, por lo que retiene patrones espec´ıficos sin generalizar bien a nuevos datos.\\n\\n6.2 Mejores pr´acticas de LSTM\\n\\n• Normalizaci´on de datos: Una de las primeras pr´acticas a implementar al entrenar una LSTM. Ayuda a mejorar la convergencia del modelo, haci´endolo m´as eficiente y evitando que el modelo quede atrapado en m´ınimos locales durante el proceso de optimizaci´on. En el caso de secuencias temporales, esto puede implicar la normalizaci´on de las features (normalmente de 0 a 1) y, al reducir la escala de las entradas, se facilita que las LSTM aprendan patrones en los datos y mantengan un mejor equilibrio en los gradientes propagados durante el entrenamiento. Sin una normalizaci´on adecuada, los gradientes pueden ser muy grandes o muy peque˜nos, lo que puede dificultar el aprendizaje de la red.\\n\\n• Uso de regulaci´on para evitar sobreajuste:\\n\\n– Dropout: Consiste en apagar aleatoriamente una fracci´on de las neuronas en cada paso del entrenamiento. Esto obliga al modelo a ser m´as robusto, porque no va a depender excesiva- mente de caracter´ısticas muy espec´ıficas.\\n\\n∗ Dropout en las conexiones recurrentes: apaga neuronas en las conexiones que hay entre capas LSTM, evitando que el modelo memorice patrones temporales innecesarios.\\n\\n∗ Dropout en las capas de salida: Se puede aplicar tras las capas Fully Connected del final de la red. Aunque es m´as com´un aplicarlo en las capas intermedias.\\n\\n– L2 regularization: esta t´ecnica penaliza los pesos muy altos del modelo haciendo que los par´ametros se mantengan bajo control y ayuda a evitar el overfitting.\\n\\nExperimentaci´on con capas bidireccionales: Las capas bidireccionales permiten al modelo\\n\\nconsiderar tanto el contexto anterior como posterior en una secuencia, lo cual es ´util en tareas donde el contexto completo mejora la precisi´on, como en el procesamiento de lenguaje natural.\\n\\nUso de modelos apilados (stacked LSTM): para mejorar la capacidad de las LSTM, se pueden\\n\\napilar m´ultiples capas LSTM. Esto permite que el modelo aprenda representaciones m´as complejas y capture mejor caracter´ısticas de datos secuenciales. Cada capa LSTM a˜nadida puede ayudar a procesar los datos de una forma m´as compleja o abstracta. Esta pr´actica aumenta la complejidad, por tanto, tambi´en incrementa los tiempos de entrenamiento y el riesgo de sobreajuste.\\n\\nAjuste de hiperpar´ametros:\\n\\n– Tama˜no del n´umero de unidades LSTM: es decir, cu´antas unidades va a tener la LSTM lo que controla la cantidad de memoria y la capacidad de aprendizaje de la red. Captura patrones m´as complejos. Aunque aumenta el riesgo de sobreajuste.\\n\\n– Tasa de aprendizaje: debe ser equilibrada, lo suficientemente peque˜na para garantizar que el modelo aprenda de manera gradual y efectiva, pero no tanto como para hacer ineficiente el proceso de entrenamiento.\\n\\n– Batch size: si los lotes son peque˜nos, aumenta la capacidad de generalizaci´on, mientras que los lotes m´as grandes aceleran el entrenamiento. Se suele experimentar o hacer validaci´on cruzada.\\n\\n8'],\n",
    "    \"\"\"Las RNNs (Redes Neuronales Recurrentes) enfrentan varias limitaciones, que se detallan a continuación:\n",
    "\n",
    "Desvanecimiento del gradiente: A medida que se propaga el error hacia atrás en el tiempo, en secuencias muy largas los gradientes se van desvaneciendo, lo que reduce el impacto del aprendizaje de los primeros pasos de la secuencia. Esto dificulta que las RNNs aprendan de elementos que están muy alejados en la secuencia. Este problema se menciona en la sección 1.1 \"Problemas de las RNN estándar\".\n",
    "Explosión del gradiente: En algunas circunstancias, los gradientes pueden crecer exponencialmente, causando inestabilidad en el entrenamiento de la red. Este efecto también es mencionado en la misma sección.\n",
    "Eficiencia a corto plazo: Las RNNs son eficaces principalmente a corto plazo, lo que significa que su rendimiento disminuye significativamente cuando las secuencias son largas o cuando la información relevante está muy separada en la secuencia. Esto es una consecuencia de los problemas de desvanecimiento del gradiente.\n",
    "Esta información se puede encontrar en la sección 1.1 \"Problemas de las RNN estándar\" (páginas 1-2 del contexto proporcionado).\"\"\",\n",
    "    \"\"\"Estas redes se enfrentan a un problema fundamental, el Desvanecimiento del gradiente. Estas\n",
    "redes se entrenan con un tipo de backpropagation, conocido como Backpropagation Through Time\n",
    "(BPTT). En este algoritmo, el gradiente se propaga hacia atras en el tiempo y se ajustan los pesos\n",
    "de la red para minimizar el error. El problema es que, a medida que el error se propaga hacia atras\n",
    "en el tiempo, si las secuencias son muy largas, los gradientes se van desvaneciendo, por lo tanto, el\n",
    "impacto del aprendizaje de los primeros pasos de la secuencia es muy pequeño. Esto hace que las RNN\n",
    "tengan dificultades para aprender de elementos que están muy lejos en la secuencia. Cabe destacar que\n",
    "también puede ocurrir el efecto contrario, la Explosión del gradiente, donde los gradientes crecen\n",
    "exponencialmente a medida que se propagan hacia atrás en el tiempo, por lo que producen inestabilidad\n",
    "en el entrenamiento de la red.\n",
    "Esta es la principal limitación de las RNNs estándar, que son eficaces a corto plazo. Pero cuando las\n",
    "secuencias son muy largas o la información relevante está muy separada en la secuencia, su rendimiento\n",
    "disminuye significativamente.\n",
    "Este problema ha impulsado que se investigue y se desarrollen nuevas arquitecturas de RNN, como\n",
    "las LSTM y las GRU, que son capaces de recordar información a largo plazo y evitar el desvanecimiento\n",
    "del gradiente.\"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a91b5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[len(df)] = [\n",
    "    \"¿Qué son las LSTM?\",\n",
    "    ['3.3 LSTM\\n\\nImagen de flujo de informaci´on en una LSTM:\\n\\nCompuerta de Compuerta de ‘Memoria a largo plazo /* datos actuales Compuerta de olvido ‘Memoria a corto plazo con datos actuales Q Xt \\\\ anes de entrada\\n\\nFigure 1: Flujo de informaci´on en una LSTM\\n\\n3.4 Flujo de informaci´on en las LSTM\\n\\nA lo largo de cada paso de tiempo de una secuencia, las LSTM pasan por una serie de actualizaciones durante las cuales las puertas deciden qu´e informaci´on se mantiene, se olvida o se utiliza de una forma m´as eficiente que las RNNs cl´asicas.\\n\\nEsta arquitectura se puede usar tanto en la traducci´on de una oraci´on, ya que puede utilizar el contexto para traducir palabras que puedan tener diferentes significados seg´un el contexto, como en la predicci´on de series temporales, ya que puede recordar informaci´on relevante a lo largo de la secuencia. Tambi´en son ´utiles para modelar dependencias a largo plazo.\\n\\n4 Comparaci´on de LSTM con RNN est´andar y GRU\\n\\n4.1 Comparaci´on entre RNN, LSTM y GRU\\n\\nPara solucionar el problema de las RNN est´andar de recordar informaci´on a largo plazo, se han desar- rollado dos arquitecturas de RNN: las LSTM y las GRU (Gated Recurrent Unit).\\n\\nEn una RNN tradicional, cada neurona toma como entrada tanto la entrada actual de la secuencia como el estado oculto de la iteraci´on anterior.\\n\\n4.2 Ventajas LSTM\\n\\nFueron introducidas por Hochreiter y Schmidhuber en 1997. Son una evoluci´on de las RNNs, dise˜nadas para resolver el problema del desvanecimiento del gradiente. Ventajas:\\n\\n5\\n\\n• Capacidad para manejar dependencias a largo plazo: Gracias a la c´elula de memoria y las puertas, las LSTM pueden recordar informaci´on durante per´ıodos prolongados, lo que las hace ideales para tareas que requieren un contexto a largo plazo.\\n\\n• Flexibilidad: Deciden de forma din´amica qu´e informaci´on mantener y qu´e informaci´on olvidar, lo que les permite adaptarse a diferentes tipos de datos y tareas.\\n\\n• Mitigaci´on del desvanecimiento del gradiente: Evitan que, durante el Backpropagation, los gradientes disminuyan progresivamente desvanci´endose.',\n",
    " '2.3 Ejemplo RNN\\n\\nEstas redes se suelen utilizar para problemas de modelado de lenguaje. Muestra un ejemplo en el que se utiliza una RNN N a N para predecir la siguiente letra de una palabra (hola) y muestra que al introducir la letra ”h”, la red calcula un estado oculto h1 y deduce la letra ”o”. Luego, al introducir la letra ”o” como input, y usando el vector de estado oculto h1, la red calcula el siguiente estado oculto h2 y deduce la letra ”l”...\\n\\n2.4 Aplicaci´on sobre im´agenes\\n\\nPara su aplicaci´on sobre im´agenes, se utiliza una red convolucional sobre la imagen para extraer las caracter´ısticas de la imagen y luego se utiliza una RNN para procesar estas caracter´ısticas y devolver el texto de salida.\\n\\nEn este caso, la ecuaci´on para calcular el estado oculto ser´ıa:\\n\\nht = tanh(whhht−1 + wxhxt + wihit) (5)\\n\\nDonde wih es la matriz de pesos que multiplicamos por el vector de caracter´ısticas extra´ıdas de la imagen it.\\n\\n3\\n\\nIntroducci´on a LSTM\\n\\n3.1 LSTM\\n\\nLas LSTMs (Long Short-Term Memory) son un tipo de RNN que fueron introducidas para resolver uno de los principales problemas de las RNN est´andar, el desvanecimiento del gradiente. El desvanecimiento del gradiente es un problema que ocurre cuando las redes neuronales tienen problemas para aprender y recordar informaci´on a lo largo de secuencias largas. Esto se debe a la disminuci´on progresiva de los gradientes que se calculan durante el entrenamiento, por lo que tienden a olvidar informaci´on a largo plazo.\\n\\nEstas redes resuelven este problema introduciendo un mecanismo de celdas de memoria, que puede retener informaci´on a lo largo de la secuencia. Esta capacidad para mantener informaci´on durante muchas iteraciones hace que las LSTM sean ideales para tareas que requieran procesamiento de datos temporales, NLP, etc.\\n\\nLas LSTM son una sofisticaci´on de las RNN tradicionales, que introducen el concepto de la celda de memoria. Esta celda de memoria es una estructura que se encarga de mantener la informaci´on relevante durante muchos pasos, evitando que se pierda o degrade la informaci´on conforme avanza la secuencia. Esta c´elula de memoria est´a controlada por una serie de mecanismos, llamados puertas que regulan la informaci´on que entra y sale de la celda de memoria y permiten que la red decida de una forma m´as eficiente qu´e informaci´on mantener y qu´e informaci´on descartar.',\n",
    " '3.2 Componentes clave de las LSTM\\n\\nLos componentes clave de las LSTM que las distinguen de las RNN est´andar son:\\n\\n• C´elula de memoria: Es el componente central de las LSTM y, a diferencia de las RNN tradi- cionales, que no tienen memoria, las LSTM est´an dise˜nadas para recordar informaci´on durante largos pasos de tiempo. La celda de memoria act´ua como un contenedor con informaci´on que puede actualizarse, olvidarse o utilizarse seg´un sea necesario. El estado de la c´elula de memoria es lo que permite a las LSTM conservar el contexto relevante durante per´ıodos m´as largos. El valor de la celda puede cambiar seg´un la red crea conveniente, lo que permite que la LSTM decida qu´e informaci´on recordar y qu´e informaci´on olvidar.\\n\\n• Puertas en LSTM: son mecanismos que controlan de manera precisa c´omo fluye la informaci´on a trav´es de la c´elula de memoria.\\n\\n– Puerta de olvido: Es un componente muy importante en la arquitectura de las LSTM ya que decide cu´anta informaci´on previa debe descartarse. Es importante para evitar que la c´elula de memoria almacene informaci´on irrelevante. Matem´aticamente, la puerta de olvido se define mediante una funci´on sigmoide f , el estado oculto anterior ht−1 y el input actual xt. Produce un valor entre 0 y 1 que determina qu´e fracci´on de la memoria anterior se mantendr´a. Si es cercano a 0, la c´elula de memoria se olvida de gran parte de la informaci´on anterior, mientras que si es cercano a 1, se mantiene casi toda la informaci´on para utilizarla en el futuro.\\n\\nft = σ(wf[ht−1,xt] + bf) (6)\\n\\nNota: f se debe a forget\\n\\n– Puerta de entrada: Es responsable de decidir qu´e nueva informaci´on debe almacenarse en la celda de memoria. Act´ua en conjunto con otro valor denominado nuevo candidato de memoria o ˜ct, que es calculada mediante una funci´on tangente hiperb´olica para generar una posible actualizaci´on de la memoria. Usa ˜ct para determinar qu´e informaci´on se va a a˜nadir a la celda de memoria. Matem´aticamente, la puerta de entrada se define mediante una funci´on sigmoide σ, el estado oculto anterior ht−1 y el input actual xt. Produce un valor entre 0 y 1, al igual que la puerta de olvido, que controla la magnitud con la que la nueva informaci´on actualiza la memoria. La combinaci´on de la puerta de entrada y con el candidato de memoria permite a las LSTM incorporar informaci´on nueva ajust´andose din´amicamente a las condiciones cambiantes de la secuencia.\\n\\nit = σ(wi[ht−1,xt] + bi) (7)\\n\\nNota: i se debe a input\\n\\n˜ct = tanh(wc[ht−1,xt] + bc) (8)\\n\\n4\\n\\n– Puerta de salida: Es la encargada de determinar qu´e parte de la informaci´on contenida dentro de la c´elula debe usarse para generar la salida de la LSTM en el instante temporal t. Esto es importante ya que no toda la informaci´on almacenada en la celda de memoria es relevante para la salida. Act´ua como un filtro que decide qu´e informaci´on debe pasarse al estado oculto ht y, por lo tanto, a la siguiente iteraci´on.\\n\\not = σ(wo[ht−1,xt] + bo) (9)\\n\\nNota: o se debe a output\\n\\nht = ot · tanh(ct) (10)\\n\\nAs´ı se obtiene el estado oculto ht, a partir de la celda de memoria ct y la puerta de salida ot.'],\n",
    "    \"\"\"Las LSTM (Long Short-Term Memory) son un tipo de redes neuronales recurrentes (RNN) que fueron introducidas para resolver el problema del desvanecimiento del gradiente, el cual afecta a las RNN estándar. Este desvanecimiento es un obstáculo que ocurre cuando las redes tienen dificultades para aprender y recordar información a lo largo de secuencias largas, debido a la disminución progresiva de los gradientes durante el entrenamiento.\n",
    "\n",
    "Las LSTM introducen un mecanismo de celdas de memoria que permite retener información a lo largo de la secuencia, lo que las hace ideales para tareas que requieren procesamiento de datos temporales, como el procesamiento del lenguaje natural (NLP) y la predicción de series temporales. Su diseño incluye puertas que regulan la información que entra y sale de la celda de memoria, permitiendo decisiones más efectivas sobre qué información mantener y qué información olvidar. Esto les confiere una capacidad superior para manejar dependencias a largo plazo en comparación con las RNN tradicionales.\n",
    "\n",
    "Esta información se puede encontrar en la sección 3.1 \"LSTM\" (páginas 3 y 4 del contexto proporcionado).\"\"\",\n",
    "    \"\"\"Las LSTMs (Long Short-Term Memory) son un tipo de RNN que fueron introducidas para resolver uno\n",
    "de los principales problemas de las RNN estándar, el desvanecimiento del gradiente. El desvanecimiento\n",
    "del gradiente es un problema que ocurre cuando las redes neuronales tienen problemas para aprender\n",
    "y recordar información a lo largo de secuencias largas. Esto se debe a la disminución progresiva de los\n",
    "gradientes que se calculan durante el entrenamiento, por lo que tienden a olvidar información a largo\n",
    "plazo.\n",
    "Estas redes resuelven este problema introduciendo un mecanismo de celdas de memoria, que puede\n",
    "retener información a lo largo de la secuencia. Esta capacidad para mantener información durante\n",
    "muchas iteraciones hace que las LSTM sean ideales para tareas que requieran procesamiento de datos\n",
    "temporales, NLP, etc.\n",
    "Las LSTM son una sofisticación de las RNN tradicionales, que introducen el concepto de la celda de\n",
    "memoria. Esta celda de memoria es una estructura que se encarga de mantener la información relevante\n",
    "durante muchos pasos, evitando que se pierda o degrade la información conforme avanza la secuencia.\n",
    "Esta célula de memoria está controlada por una serie de mecanismos, llamados puertas que regulan la\n",
    "información que entra y sale de la celda de memoria y permiten que la red decida de una forma más\n",
    "eficiente qué información mantener y qué información descartar.\"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88c33d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[len(df)] = [\n",
    "    \"¿Cuáles son las componentes clave de las LSTM?\",\n",
    "    ['3.2 Componentes clave de las LSTM\\n\\nLos componentes clave de las LSTM que las distinguen de las RNN est´andar son:\\n\\n• C´elula de memoria: Es el componente central de las LSTM y, a diferencia de las RNN tradi- cionales, que no tienen memoria, las LSTM est´an dise˜nadas para recordar informaci´on durante largos pasos de tiempo. La celda de memoria act´ua como un contenedor con informaci´on que puede actualizarse, olvidarse o utilizarse seg´un sea necesario. El estado de la c´elula de memoria es lo que permite a las LSTM conservar el contexto relevante durante per´ıodos m´as largos. El valor de la celda puede cambiar seg´un la red crea conveniente, lo que permite que la LSTM decida qu´e informaci´on recordar y qu´e informaci´on olvidar.\\n\\n• Puertas en LSTM: son mecanismos que controlan de manera precisa c´omo fluye la informaci´on a trav´es de la c´elula de memoria.\\n\\n– Puerta de olvido: Es un componente muy importante en la arquitectura de las LSTM ya que decide cu´anta informaci´on previa debe descartarse. Es importante para evitar que la c´elula de memoria almacene informaci´on irrelevante. Matem´aticamente, la puerta de olvido se define mediante una funci´on sigmoide f , el estado oculto anterior ht−1 y el input actual xt. Produce un valor entre 0 y 1 que determina qu´e fracci´on de la memoria anterior se mantendr´a. Si es cercano a 0, la c´elula de memoria se olvida de gran parte de la informaci´on anterior, mientras que si es cercano a 1, se mantiene casi toda la informaci´on para utilizarla en el futuro.\\n\\nft = σ(wf[ht−1,xt] + bf) (6)\\n\\nNota: f se debe a forget\\n\\n– Puerta de entrada: Es responsable de decidir qu´e nueva informaci´on debe almacenarse en la celda de memoria. Act´ua en conjunto con otro valor denominado nuevo candidato de memoria o ˜ct, que es calculada mediante una funci´on tangente hiperb´olica para generar una posible actualizaci´on de la memoria. Usa ˜ct para determinar qu´e informaci´on se va a a˜nadir a la celda de memoria. Matem´aticamente, la puerta de entrada se define mediante una funci´on sigmoide σ, el estado oculto anterior ht−1 y el input actual xt. Produce un valor entre 0 y 1, al igual que la puerta de olvido, que controla la magnitud con la que la nueva informaci´on actualiza la memoria. La combinaci´on de la puerta de entrada y con el candidato de memoria permite a las LSTM incorporar informaci´on nueva ajust´andose din´amicamente a las condiciones cambiantes de la secuencia.\\n\\nit = σ(wi[ht−1,xt] + bi) (7)\\n\\nNota: i se debe a input\\n\\n˜ct = tanh(wc[ht−1,xt] + bc) (8)\\n\\n4\\n\\n– Puerta de salida: Es la encargada de determinar qu´e parte de la informaci´on contenida dentro de la c´elula debe usarse para generar la salida de la LSTM en el instante temporal t. Esto es importante ya que no toda la informaci´on almacenada en la celda de memoria es relevante para la salida. Act´ua como un filtro que decide qu´e informaci´on debe pasarse al estado oculto ht y, por lo tanto, a la siguiente iteraci´on.\\n\\not = σ(wo[ht−1,xt] + bo) (9)\\n\\nNota: o se debe a output\\n\\nht = ot · tanh(ct) (10)\\n\\nAs´ı se obtiene el estado oculto ht, a partir de la celda de memoria ct y la puerta de salida ot.',\n",
    " '3.3 LSTM\\n\\nImagen de flujo de informaci´on en una LSTM:\\n\\nCompuerta de Compuerta de ‘Memoria a largo plazo /* datos actuales Compuerta de olvido ‘Memoria a corto plazo con datos actuales Q Xt \\\\ anes de entrada\\n\\nFigure 1: Flujo de informaci´on en una LSTM\\n\\n3.4 Flujo de informaci´on en las LSTM\\n\\nA lo largo de cada paso de tiempo de una secuencia, las LSTM pasan por una serie de actualizaciones durante las cuales las puertas deciden qu´e informaci´on se mantiene, se olvida o se utiliza de una forma m´as eficiente que las RNNs cl´asicas.\\n\\nEsta arquitectura se puede usar tanto en la traducci´on de una oraci´on, ya que puede utilizar el contexto para traducir palabras que puedan tener diferentes significados seg´un el contexto, como en la predicci´on de series temporales, ya que puede recordar informaci´on relevante a lo largo de la secuencia. Tambi´en son ´utiles para modelar dependencias a largo plazo.\\n\\n4 Comparaci´on de LSTM con RNN est´andar y GRU\\n\\n4.1 Comparaci´on entre RNN, LSTM y GRU\\n\\nPara solucionar el problema de las RNN est´andar de recordar informaci´on a largo plazo, se han desar- rollado dos arquitecturas de RNN: las LSTM y las GRU (Gated Recurrent Unit).\\n\\nEn una RNN tradicional, cada neurona toma como entrada tanto la entrada actual de la secuencia como el estado oculto de la iteraci´on anterior.\\n\\n4.2 Ventajas LSTM\\n\\nFueron introducidas por Hochreiter y Schmidhuber en 1997. Son una evoluci´on de las RNNs, dise˜nadas para resolver el problema del desvanecimiento del gradiente. Ventajas:\\n\\n5\\n\\n• Capacidad para manejar dependencias a largo plazo: Gracias a la c´elula de memoria y las puertas, las LSTM pueden recordar informaci´on durante per´ıodos prolongados, lo que las hace ideales para tareas que requieren un contexto a largo plazo.\\n\\n• Flexibilidad: Deciden de forma din´amica qu´e informaci´on mantener y qu´e informaci´on olvidar, lo que les permite adaptarse a diferentes tipos de datos y tareas.\\n\\n• Mitigaci´on del desvanecimiento del gradiente: Evitan que, durante el Backpropagation, los gradientes disminuyan progresivamente desvanci´endose.',\n",
    " 'LSTM\\n\\nContents\\n\\n1.1 Problemas de las RNN est´andar . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.1 Vanilla RNN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.2 Flujo de RNN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.3 Ejemplo RNN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.4 Aplicaci´on sobre im´agenes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.1 LSTM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2 Componentes clave de las LSTM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3 LSTM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.4 Flujo de informaci´on en las LSTM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 Comparaci´on de LSTM con RNN est´andar y GRU 4.1 Comparaci´on entre RNN, LSTM y GRU . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2 Ventajas LSTM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.3 GRU . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.4 Comparaci´on entre LSTM y GRU . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.5 Resumen comparativo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.1 Casos de uso de las LSTM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 Limitaciones y mejores pr´acticas de LSTM 6.1 Limitaciones de las LSTM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.2 Mejores pr´acticas de LSTM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 2 2 3 3 3 3 4 4 4 5 5 5 5 5 6 6 7 7 7 8 8 8\\n\\n1 Introducci´on a RNN\\n\\n2 RNN\\n\\n3 Introducci´on a LSTM\\n\\n5 ¿Cu´ando usar LSTM?\\n\\n1\\n\\n1 Introducci´on a RNN\\n\\nLas Redes Neuronales Recurrentes son un tipo espec´ıfico de arquitectura de red neuronal que est´a dise˜nado para manejar datos secuenciales, lo que significa que el orden de los datos es muy impor- tante. Este tipo de red es fundamental en tareas donde la informaci´on anterior debe ser recordada o procesada junto a la informaci´on actual. A lo largo de la historia de la IA, las RNN han sido muy impor- tantes a la hora de resolver problemas relacionados con el an´alisis de frecuencias como NLP, predicciones de series temporales, transcripci´on de voz a texto o modelado de v´ıdeo y audio.\\n\\nEn las redes convencionales como las feedforward, la salida solo depende del input actual. Sin embargo, en las RNN, la salida depende tanto del input actual como de los inputs anteriores al introducir el concepto de memoria. Esta capacidad de recordar informaci´on previa es lo que hace que las RNN sean tan poderosas en la resoluci´on de problemas secuenciales.'],\n",
    "    \"\"\"Las componentes clave de las LSTM son:\n",
    "\n",
    "Célula de memoria: Este es el componente central que permite a las LSTM recordar información durante largos pasos de tiempo. Actúa como un contenedor que puede actualizarse, olvidarse o utilizarse según sea necesario, permitiendo conservar el contexto relevante durante períodos prolongados.\n",
    "Puertas en LSTM: Son mecanismos que controlan cómo fluye la información a través de la célula de memoria:\n",
    "Puerta de olvido: Decide cuánta información previa debe descartarse, evitando que se almacene información irrelevante.\n",
    "Puerta de entrada: Decide qué nueva información debe almacenarse en la célula de memoria y trabaja en conjunto con un nuevo candidato de memoria para poder actualizar la memoria de manera dinámica.\n",
    "Puerta de salida: Determina qué parte de la información en la célula de memoria se debe utilizar para generar la salida de la LSTM en un instante temporal.\n",
    "Esta información se encuentra en la sección 3.2 \"Componentes clave de las LSTM\" (páginas 3 y 4 del contexto proporcionado).\"\"\",\n",
    "    \"\"\"Los componentes clave de las LSTM que las distinguen de las RNN estándar son:\n",
    "• Célula de memoria: Es el componente central de las LSTM y, a diferencia de las RNN tradicionales,\n",
    "que no tienen memoria, las LSTM están diseñaadas para recordar información durante\n",
    "largos pasos de tiempo. La celda de memoria actúa como un contenedor con información que\n",
    "puede actualizarse, olvidarse o utilizarse según sea necesario. El estado de la célula de memoria es\n",
    "lo que permite a las LSTM conservar el contexto relevante durante períodos más largos. El valor\n",
    "de la celda puede cambiar según la red crea conveniente, lo que permite que la LSTM decida qué\n",
    "información recordar y qué información olvidar.\n",
    "• Puertas en LSTM: son mecanismos que controlan de manera precisa cómo fluye la inforinformaciónmación\n",
    "a través de la célula de memoria.\n",
    "- Puerta de olvido: Es un componente muy importante en la arquitectura de las LSTM ya que\n",
    "decide cuánta información previa debe descartarse. Es importante para evitar que la célula de\n",
    "memoria almacene información irrelevante. Matemáticamente, la puerta de olvido se define\n",
    "mediante una función sigmoide f , el estado oculto anterior ht−1 y el input actual xt. Produce\n",
    "un valor entre 0 y 1 que determina qué fracción de la memoria anterior se mantendrá. Si es\n",
    "cercano a 0, la célula de memoria se olvida de gran parte de la información anterior, mientras\n",
    "que si es cercano a 1, se mantiene casi toda la información para utilizarla en el futuro.\n",
    "\n",
    "- Puerta de entrada: Es responsable de decidir qué nueva información debe almacenarse\n",
    "en la celda de memoria. Actúa en conjunto con otro valor denominado nuevo candidato\n",
    "de memoria o ˜ ct, que es calculada mediante una función tangente hiperbólica para generar\n",
    "una posible actualización de la memoria. Usa ˜ ct para determinar qué información se va a\n",
    "añadir a la celda de memoria. Matemáticamente, la puerta de entrada se define mediante\n",
    "una función sigmoide σ, el estado oculto anterior ht−1 y el input actual xt. Produce un valor\n",
    "entre 0 y 1, al igual que la puerta de olvido, que controla la magnitud con la que la nueva\n",
    "información actualiza la memoria. La combinación de la puerta de entrada y con el candidato\n",
    "de memoria permite a las LSTM incorporar información nueva ajustándose dinámicamente a\n",
    "las condiciones cambiantes de la secuencia.\n",
    "\n",
    "- Puerta de salida: Es la encargada de determinar qué parte de la información contenida\n",
    "dentro de la célula debe usarse para generar la salida de la LSTM en el instante temporal\n",
    "t. Esto es importante ya que no toda la información almacenada en la celda de memoria es\n",
    "relevante para la salida. Actúa como un filtro que decide qué información debe pasarse al\n",
    "estado oculto ht y, por lo tanto, a la siguiente iteración.\"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9384f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[len(df)] = [\n",
    "    \"¿Qué son las redes GRU?\",\n",
    "    ['4.3 GRU\\n\\nLas Gated Recurrent Units (GRU) son un tipo de RNN que fueron introducidas en 2014 y surgen como una simplificaci´on de las LSTM. Est´an dise˜nadas para solucionar los problemas de las RNNs con el desvanecimiento del gradiente y la dificultad para recordar informaci´on a largo plazo, pero con una arquitectura m´as simple que las LSTM. En vez de tener 3 puertas (olvido, entrada y salida), las GRU tienen solo 2 puertas (puerta de actualizaci´on y puerta de reinicio). La puerta de actualizaci´on combina las funciones de la puerta de olvido y la puerta de entrada de las LSTM, ya que decide cu´anta informaci´on de la memoria anterior debe retenerse y cu´anta nueva informaci´on debe a˜nadirse. La puerta de reinicio controla cu´anto de la memoria anterior debe tenerse en cuenta para calcular el nuevo estado oculto o salida; tiene una funci´on similar a la puerta de salida de las LSTM.\\n\\n4.4 Comparaci´on entre LSTM y GRU\\n\\n• Complejidad: Es la principal diferencia entre ambas arquitecturas. Las LSTM tienen 3 puertas y una c´elula de memoria, mientras que las GRU tienen 2 puertas y no tienen una c´elula de memoria separada. Esto hace que las GRU sean m´as simples y, por lo general, m´as r´apidas de entrenar que las LSTM.\\n\\n• Rendimiento en secuencias largas: Ambas arquitecturas son buenas para manejar depen- dencias a largo plazo, pero algunos estudios sugieren que las LSTM pueden ser ligeramente m´as efectivas en tareas en las que las secuencias de datos son muy largas o complejas, ya que al tener la c´elula de memoria permite un mayor control sobre la informaci´on que se retiene y se olvida.\\n\\n• Coste computacional: Las GRU suelen ser m´as r´apidas y computacionalmente eficientes que las LSTM, lo que las convierte en una opci´on preferida en tareas en las que el tiempo de entrenamiento y la eficiencia de c´alculo son cr´ıticos.\\n\\n• Aplicaciones y rendimiento: En t´erminos generales, las LSTM y las GRU tienden a ofrecer rendimientos similares. En algunos casos, las GRU han demostrado funcionar mejor cuando se dispone de menos datos o cuando las dependencias temporales no son extremadamente complejas.\\n\\n• Capacidad de modelado: Las LSTM, gracias a su arquitectura m´as compleja, pueden ser m´as adecuadas para problemas donde se necesita modelar dependencias muy largas y donde la red debe ser extremadamente precisa en la memoria. Por otro lado, las GRU pueden ofrecer resultados similares en problemas con menos requerimientos de memoria a largo plazo.\\n\\n6',\n",
    " '1.1 Problemas de las RNN est´andar\\n\\nEstas redes se enfrentan a un problema fundamental, el Desvanecimiento del gradiente. Estas redes se entrenan con un tipo de backpropagation, conocido como Backpropagation Through Time (BPTT). En este algoritmo, el gradiente se propaga hacia atr´as en el tiempo y se ajustan los pesos de la red para minimizar el error. El problema es que, a medida que el error se propaga hacia atr´as en el tiempo, si las secuencias son muy largas, los gradientes se van desvaneciendo, por lo tanto, el impacto del aprendizaje de los primeros pasos de la secuencia es muy peque˜no. Esto hace que las RNN tengan dificultades para aprender de elementos que est´an muy lejos en la secuencia. Cabe destacar que tambi´en puede ocurrir el efecto contrario, la Explosi´on del gradiente, donde los gradientes crecen exponencialmente a medida que se propagan hacia atr´as en el tiempo, por lo que producen inestabilidad en el entrenamiento de la red.\\n\\nEsta es la principal limitaci´on de las RNNs est´andar, que son eficaces a corto plazo. Pero cuando las secuencias son muy largas o la informaci´on relevante est´a muy separada en la secuencia, su rendimiento disminuye significativamente.\\n\\nEste problema ha impulsado que se investigue y se desarrollen nuevas arquitecturas de RNN, como las LSTM y las GRU, que son capaces de recordar informaci´on a largo plazo y evitar el desvanecimiento del gradiente.\\n\\n2 RNN\\n\\nEstas redes permiten usar una variedad m´as amplia de estructuras tanto de entrada como de salida:\\n\\n• 1 a 1: Redes convencionales, un solo input y un solo output.\\n\\n• 1 a N: Por ejemplo: Generaci´on de texto a partir de una imagen.\\n\\nN a 1: Por ejemplo: An´alisis de sentimientos a partir de una rese˜na (de N palabras).\\n\\nN a N: Por ejemplo: Traducci´on de un idioma a otro. Tiene N palabras de entrada y M palabras\\n\\nde salida ya que la traducci´on puede tener m´as o menos palabras que el texto original por lo que t´ecnicamente es N a M.\\n\\nLa funci´on matem´atica que define una RNN es la siguiente:\\n\\nht = fω(ht−1,xt) (1)\\n\\nDonde:\\n\\nht es el estado oculto en el tiempo t.\\n\\nfω es la funci´on de activaci´on. Depende de los pesos ω de la red.\\n\\nht−1 es el estado oculto en el instante anterior, t − 1.\\n\\nxt es el input en el tiempo t.\\n\\n2',\n",
    " '3.3 LSTM\\n\\nImagen de flujo de informaci´on en una LSTM:\\n\\nCompuerta de Compuerta de ‘Memoria a largo plazo /* datos actuales Compuerta de olvido ‘Memoria a corto plazo con datos actuales Q Xt \\\\ anes de entrada\\n\\nFigure 1: Flujo de informaci´on en una LSTM\\n\\n3.4 Flujo de informaci´on en las LSTM\\n\\nA lo largo de cada paso de tiempo de una secuencia, las LSTM pasan por una serie de actualizaciones durante las cuales las puertas deciden qu´e informaci´on se mantiene, se olvida o se utiliza de una forma m´as eficiente que las RNNs cl´asicas.\\n\\nEsta arquitectura se puede usar tanto en la traducci´on de una oraci´on, ya que puede utilizar el contexto para traducir palabras que puedan tener diferentes significados seg´un el contexto, como en la predicci´on de series temporales, ya que puede recordar informaci´on relevante a lo largo de la secuencia. Tambi´en son ´utiles para modelar dependencias a largo plazo.\\n\\n4 Comparaci´on de LSTM con RNN est´andar y GRU\\n\\n4.1 Comparaci´on entre RNN, LSTM y GRU\\n\\nPara solucionar el problema de las RNN est´andar de recordar informaci´on a largo plazo, se han desar- rollado dos arquitecturas de RNN: las LSTM y las GRU (Gated Recurrent Unit).\\n\\nEn una RNN tradicional, cada neurona toma como entrada tanto la entrada actual de la secuencia como el estado oculto de la iteraci´on anterior.\\n\\n4.2 Ventajas LSTM\\n\\nFueron introducidas por Hochreiter y Schmidhuber en 1997. Son una evoluci´on de las RNNs, dise˜nadas para resolver el problema del desvanecimiento del gradiente. Ventajas:\\n\\n5\\n\\n• Capacidad para manejar dependencias a largo plazo: Gracias a la c´elula de memoria y las puertas, las LSTM pueden recordar informaci´on durante per´ıodos prolongados, lo que las hace ideales para tareas que requieren un contexto a largo plazo.\\n\\n• Flexibilidad: Deciden de forma din´amica qu´e informaci´on mantener y qu´e informaci´on olvidar, lo que les permite adaptarse a diferentes tipos de datos y tareas.\\n\\n• Mitigaci´on del desvanecimiento del gradiente: Evitan que, durante el Backpropagation, los gradientes disminuyan progresivamente desvanci´endose.'],\n",
    "    \"\"\"Las Gated Recurrent Units (GRU) son un tipo de redes neuronales recurrentes (RNN) que fueron introducidas en 2014 como una simplificación de las LSTM. Están diseñadas para solucionar los problemas de las RNNs relacionados con el desvanecimiento del gradiente y la dificultad de recordar información a largo plazo, pero con una arquitectura más simple. A diferencia de las LSTM, que tienen 3 puertas (olvido, entrada y salida), las GRU tienen solo 2 puertas: la puerta de actualización y la puerta de reinicio. La puerta de actualización combina las funciones de la puerta de olvido y la puerta de entrada de las LSTM, mientras que la puerta de reinicio controla cuánto de la memoria anterior se debe tener en cuenta para calcular el nuevo estado oculto.\n",
    "\n",
    "Esta información se encuentra en la sección 4.3 \"GRU\" (páginas 3 y 4 del contexto proporcionado).\"\"\",\n",
    "    \"\"\"Las Gated Recurrent Units (GRU) son un tipo de RNN que fueron introducidas en 2014 y surgen como\n",
    "una simplificación de las LSTM. Están diseñnadas para solucionar los problemas de las RNNs con el\n",
    "desvanecimiento del gradiente y la dificultad para recordar información a largo plazo, pero con una\n",
    "arquitectura más simple que las LSTM. En vez de tener 3 puertas (olvido, entrada y salida), las GRU\n",
    "tienen solo 2 puertas (puerta de actualización y puerta de reinicio). La puerta de actualización combina\n",
    "las funciones de la puerta de olvido y la puerta de entrada de las LSTM, ya que decide cuánta información\n",
    "de la memoria anterior debe retenerse y cuánta nueva información debe añadirse. La puerta de reinicio\n",
    "controla cuánto de la memoria anterior debe tenerse en cuenta para calcular el nuevo estado oculto o\n",
    "salida; tiene una función similar a la puerta de salida de las LSTM.\"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dfde40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[len(df)] = [\n",
    "    \"Proporcióname una comparación entre las LSTM y las GRU\",\n",
    "    ['4.3 GRU\\n\\nLas Gated Recurrent Units (GRU) son un tipo de RNN que fueron introducidas en 2014 y surgen como una simplificaci´on de las LSTM. Est´an dise˜nadas para solucionar los problemas de las RNNs con el desvanecimiento del gradiente y la dificultad para recordar informaci´on a largo plazo, pero con una arquitectura m´as simple que las LSTM. En vez de tener 3 puertas (olvido, entrada y salida), las GRU tienen solo 2 puertas (puerta de actualizaci´on y puerta de reinicio). La puerta de actualizaci´on combina las funciones de la puerta de olvido y la puerta de entrada de las LSTM, ya que decide cu´anta informaci´on de la memoria anterior debe retenerse y cu´anta nueva informaci´on debe a˜nadirse. La puerta de reinicio controla cu´anto de la memoria anterior debe tenerse en cuenta para calcular el nuevo estado oculto o salida; tiene una funci´on similar a la puerta de salida de las LSTM.\\n\\n4.4 Comparaci´on entre LSTM y GRU\\n\\n• Complejidad: Es la principal diferencia entre ambas arquitecturas. Las LSTM tienen 3 puertas y una c´elula de memoria, mientras que las GRU tienen 2 puertas y no tienen una c´elula de memoria separada. Esto hace que las GRU sean m´as simples y, por lo general, m´as r´apidas de entrenar que las LSTM.\\n\\n• Rendimiento en secuencias largas: Ambas arquitecturas son buenas para manejar depen- dencias a largo plazo, pero algunos estudios sugieren que las LSTM pueden ser ligeramente m´as efectivas en tareas en las que las secuencias de datos son muy largas o complejas, ya que al tener la c´elula de memoria permite un mayor control sobre la informaci´on que se retiene y se olvida.\\n\\n• Coste computacional: Las GRU suelen ser m´as r´apidas y computacionalmente eficientes que las LSTM, lo que las convierte en una opci´on preferida en tareas en las que el tiempo de entrenamiento y la eficiencia de c´alculo son cr´ıticos.\\n\\n• Aplicaciones y rendimiento: En t´erminos generales, las LSTM y las GRU tienden a ofrecer rendimientos similares. En algunos casos, las GRU han demostrado funcionar mejor cuando se dispone de menos datos o cuando las dependencias temporales no son extremadamente complejas.\\n\\n• Capacidad de modelado: Las LSTM, gracias a su arquitectura m´as compleja, pueden ser m´as adecuadas para problemas donde se necesita modelar dependencias muy largas y donde la red debe ser extremadamente precisa en la memoria. Por otro lado, las GRU pueden ofrecer resultados similares en problemas con menos requerimientos de memoria a largo plazo.\\n\\n6',\n",
    " '4.5 Resumen comparativo\\n\\nCaracter´ıstica RNN est´andar LSTM GRU Memoria a largo Limitada Excelente Muy buena plazo Problema del gradi- Desvanecimiento Mitigado Mitigado ente Complejidad Baja Alta Media Velocidad de entre- R´apida (en secuencias Lenta (debido a m´as M´as r´apida que LSTM namiento cortas) puertas) N´umero de puertas N/A 3 (olvido, entrada, sal- 2 (actualizaci´on, reini- ida) cio) Eficiencia computa- Alta en secuencias cor- Menos eficiente M´as eficiente que cional tas LSTM Aplicaciones Secuencias cortas Secuencias largas y Secuencias de longitud complejas media/larga\\n\\nTable 1: Comparaci´on entre RNN est´andar, LSTM y GRU\\n\\n¿Cu´ando usar LSTM?\\n\\n5.1 Casos de uso de las LSTM\\n\\nDependencias temporales a largo plazo: uno de los puntos m´as fuertes de las LSTM es su\\n\\ncapacidad de recordar informaci´on en largos periodos de tiempo.\\n\\n– Traducci´on autom´atica: ya que para entender el significado de una palabra o frase es impor- tante tener en cuenta el contexto.\\n\\n– Generaci´on de textos: ya que para mantener el estilo, la consistencia y la coherencia del texto es importante tener en cuenta lo generado anteriormente.\\n\\n– Reconocimiento de voz: ya que es capaz de identificar patrones pronunciados anteriormente y relacionarlos con el contexto actual.\\n\\nProblemas con alta complejidad temporal: cuando los datos secuenciales tienen relaciones\\n\\ntemporales complejas, las LSTM pueden ser muy ´utiles ya que, gracias a su c´elula de memoria y sus puertas, pueden seleccionar qu´e informaci´on es relevante y cu´al no, ajust´andose as´ı a la complejidad de los datos.\\n\\n– Predicci´on de series temporales: como acciones o el precio de la energ´ıa que vienen influenci- ados por factores actuales pero tambi´en por el estado del mercado en el pasado.\\n\\n– Control rob´otico o de sistemas:\\n\\n• Datos ruidosos o incompletos:\\n\\n– Sensores en tiempo real:\\n\\n– Medicina y diagn´ostico:\\n\\nTareas que requieren precisi´on en la memoria y contexto:\\n\\n– Modelado de lenguaje natural:\\n\\n– An´alisis de videos:\\n\\nCuando las RNN est´andar no son suficientes:\\n\\n– Predicci´on de precios:\\n\\n– Reconocimiento de patrones en series temporales complejas:\\n\\n7',\n",
    " '3.3 LSTM\\n\\nImagen de flujo de informaci´on en una LSTM:\\n\\nCompuerta de Compuerta de ‘Memoria a largo plazo /* datos actuales Compuerta de olvido ‘Memoria a corto plazo con datos actuales Q Xt \\\\ anes de entrada\\n\\nFigure 1: Flujo de informaci´on en una LSTM\\n\\n3.4 Flujo de informaci´on en las LSTM\\n\\nA lo largo de cada paso de tiempo de una secuencia, las LSTM pasan por una serie de actualizaciones durante las cuales las puertas deciden qu´e informaci´on se mantiene, se olvida o se utiliza de una forma m´as eficiente que las RNNs cl´asicas.\\n\\nEsta arquitectura se puede usar tanto en la traducci´on de una oraci´on, ya que puede utilizar el contexto para traducir palabras que puedan tener diferentes significados seg´un el contexto, como en la predicci´on de series temporales, ya que puede recordar informaci´on relevante a lo largo de la secuencia. Tambi´en son ´utiles para modelar dependencias a largo plazo.\\n\\n4 Comparaci´on de LSTM con RNN est´andar y GRU\\n\\n4.1 Comparaci´on entre RNN, LSTM y GRU\\n\\nPara solucionar el problema de las RNN est´andar de recordar informaci´on a largo plazo, se han desar- rollado dos arquitecturas de RNN: las LSTM y las GRU (Gated Recurrent Unit).\\n\\nEn una RNN tradicional, cada neurona toma como entrada tanto la entrada actual de la secuencia como el estado oculto de la iteraci´on anterior.\\n\\n4.2 Ventajas LSTM\\n\\nFueron introducidas por Hochreiter y Schmidhuber en 1997. Son una evoluci´on de las RNNs, dise˜nadas para resolver el problema del desvanecimiento del gradiente. Ventajas:\\n\\n5\\n\\n• Capacidad para manejar dependencias a largo plazo: Gracias a la c´elula de memoria y las puertas, las LSTM pueden recordar informaci´on durante per´ıodos prolongados, lo que las hace ideales para tareas que requieren un contexto a largo plazo.\\n\\n• Flexibilidad: Deciden de forma din´amica qu´e informaci´on mantener y qu´e informaci´on olvidar, lo que les permite adaptarse a diferentes tipos de datos y tareas.\\n\\n• Mitigaci´on del desvanecimiento del gradiente: Evitan que, durante el Backpropagation, los gradientes disminuyan progresivamente desvanci´endose.'],\n",
    "    \"\"\"Las LSTM (Long Short-Term Memory) y las GRU (Gated Recurrent Units) tienen varias diferencias clave:\n",
    "\n",
    "Complejidad:\n",
    "Las LSTM tienen 3 puertas (olvido, entrada y salida) y una célula de memoria, lo que las hace más complejas.\n",
    "Las GRU tienen solo 2 puertas (puerta de actualización y puerta de reinicio) y no tienen una célula de memoria separada, lo que las hace más simples y, generalmente, más rápidas de entrenar.\n",
    "(Sección 4.4 Comparación entre LSTM y GRU)\n",
    "Rendimiento en secuencias largas:\n",
    "Ambas arquitecturas son eficaces para manejar dependencias a largo plazo, pero algunos estudios sugieren que las LSTM pueden ser ligeramente más efectivas en tareas con secuencias muy largas o complejas debido a su célula de memoria que permite un mayor control sobre la retención y el olvido de información.\n",
    "(Sección 4.4 Comparación entre LSTM y GRU)\n",
    "Coste computacional:\n",
    "Las GRU suelen ser más rápidas y computacionalmente eficientes que las LSTM, lo que las convierte en una opción preferida en tareas donde el tiempo de entrenamiento y la eficiencia de cálculo son críticos.\n",
    "(Sección 4.4 Comparación entre LSTM y GRU)\n",
    "Aplicaciones y rendimiento:\n",
    "En términos generales, LSTM y GRU tienden a ofrecer rendimientos similares. Sin embargo, las GRU han demostrado funcionar mejor con menos datos o cuando las dependencias temporales no son extremadamente complejas.\n",
    "(Sección 4.4 Comparación entre LSTM y GRU)\n",
    "Capacidad de modelado:\n",
    "Las LSTM pueden ser más adecuadas para problemas que requieren modelar dependencias muy largas y precisas en la memoria, mientras que las GRU pueden ofrecer resultados similares en problemas con menos requerimientos de memoria a largo plazo.\n",
    "(Sección 4.4 Comparación entre LSTM y GRU)\n",
    "Esta comparación se resume en la tabla de la sección 4.5, que aborda diversas características de RNN estándar, LSTM y GRU.\"\"\",\n",
    "    \"\"\"Comparación entre LSTM y GRU\n",
    "• Complejidad: Es la principal diferencia entre ambas arquitecturas. Las LSTM tienen 3 puertas y\n",
    "una célula de memoria, mientras que las GRU tienen 2 puertas y no tienen una célula de memoria\n",
    "separada. Esto hace que las GRU sean más simples y, por lo general, más rápidas de entrenar que\n",
    "las LSTM.\n",
    "• Rendimiento en secuencias largas: Ambas arquitecturas son buenas para manejar dependencias\n",
    "a largo plazo, pero algunos estudios sugieren que las LSTM pueden ser ligeramente más\n",
    "efectivas en tareas en las que las secuencias de datos son muy largas o complejas, ya que al tener\n",
    "la célula de memoria permite un mayor control sobre la información que se retiene y se olvida.\n",
    "• Coste computacional: Las GRU suelen ser más rápidas y computacionalmente eficientes que las\n",
    "LSTM, lo que las convierte en una opción preferida en tareas en las que el tiempo de entrenamiento\n",
    "y la eficiencia de cálculo son críticos.\n",
    "• Aplicaciones y rendimiento: En términos generales, las LSTM y las GRU tienden a ofrecer\n",
    "rendimientos similares. En algunos casos, las GRU han demostrado funcionar mejor cuando se\n",
    "dispone de menos datos o cuando las dependencias temporales no son extremadamente complejas.\n",
    "• Capacidad de modelado: Las LSTM, gracias a su arquitectura más compleja, pueden ser más\n",
    "adecuadas para problemas donde se necesita modelar dependencias muy largas y donde la red debe\n",
    "ser extremadamente precisa en la memoria. Por otro lado, las GRU pueden ofrecer resultados\n",
    "similares en problemas con menos requerimientos de memoria a largo plazo.\"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e215894",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[len(df)] = [\n",
    "    \"¿Cuáles son las limitaciones de las LSTM?\",\n",
    "    ['6 Limitaciones y mejores pr´acticas de LSTM\\n\\n6.1 Limitaciones de las LSTM\\n\\n• Complejidad y coste computacional: las puertas y la c´elula de memoria a˜naden complejidad a la red, adem´as de aumentar el coste computacional y el tiempo de entrenamiento. As´ı que en casos en los que haya recursos limitados o se necesite un entrenamiento r´apido, las LSTM pueden no ser la mejor opci´on.\\n\\n• Problemas con secuencias extremadamente largas: aunque son bastante mejores que las RNNs en escenarios donde las secuencias son extremadamente largas, pueden seguir teniendo prob- lemas con el desvanecimiento del gradiente a muy largo plazo perdiendo informaci´on relevante.\\n\\n• Riesgo de sobreajuste: este riesgo es considerable en conjuntos de datos muy peque˜nos, por lo que retiene patrones espec´ıficos sin generalizar bien a nuevos datos.\\n\\n6.2 Mejores pr´acticas de LSTM\\n\\n• Normalizaci´on de datos: Una de las primeras pr´acticas a implementar al entrenar una LSTM. Ayuda a mejorar la convergencia del modelo, haci´endolo m´as eficiente y evitando que el modelo quede atrapado en m´ınimos locales durante el proceso de optimizaci´on. En el caso de secuencias temporales, esto puede implicar la normalizaci´on de las features (normalmente de 0 a 1) y, al reducir la escala de las entradas, se facilita que las LSTM aprendan patrones en los datos y mantengan un mejor equilibrio en los gradientes propagados durante el entrenamiento. Sin una normalizaci´on adecuada, los gradientes pueden ser muy grandes o muy peque˜nos, lo que puede dificultar el aprendizaje de la red.\\n\\n• Uso de regulaci´on para evitar sobreajuste:\\n\\n– Dropout: Consiste en apagar aleatoriamente una fracci´on de las neuronas en cada paso del entrenamiento. Esto obliga al modelo a ser m´as robusto, porque no va a depender excesiva- mente de caracter´ısticas muy espec´ıficas.\\n\\n∗ Dropout en las conexiones recurrentes: apaga neuronas en las conexiones que hay entre capas LSTM, evitando que el modelo memorice patrones temporales innecesarios.\\n\\n∗ Dropout en las capas de salida: Se puede aplicar tras las capas Fully Connected del final de la red. Aunque es m´as com´un aplicarlo en las capas intermedias.\\n\\n– L2 regularization: esta t´ecnica penaliza los pesos muy altos del modelo haciendo que los par´ametros se mantengan bajo control y ayuda a evitar el overfitting.\\n\\nExperimentaci´on con capas bidireccionales: Las capas bidireccionales permiten al modelo\\n\\nconsiderar tanto el contexto anterior como posterior en una secuencia, lo cual es ´util en tareas donde el contexto completo mejora la precisi´on, como en el procesamiento de lenguaje natural.\\n\\nUso de modelos apilados (stacked LSTM): para mejorar la capacidad de las LSTM, se pueden\\n\\napilar m´ultiples capas LSTM. Esto permite que el modelo aprenda representaciones m´as complejas y capture mejor caracter´ısticas de datos secuenciales. Cada capa LSTM a˜nadida puede ayudar a procesar los datos de una forma m´as compleja o abstracta. Esta pr´actica aumenta la complejidad, por tanto, tambi´en incrementa los tiempos de entrenamiento y el riesgo de sobreajuste.\\n\\nAjuste de hiperpar´ametros:\\n\\n– Tama˜no del n´umero de unidades LSTM: es decir, cu´antas unidades va a tener la LSTM lo que controla la cantidad de memoria y la capacidad de aprendizaje de la red. Captura patrones m´as complejos. Aunque aumenta el riesgo de sobreajuste.\\n\\n– Tasa de aprendizaje: debe ser equilibrada, lo suficientemente peque˜na para garantizar que el modelo aprenda de manera gradual y efectiva, pero no tanto como para hacer ineficiente el proceso de entrenamiento.\\n\\n– Batch size: si los lotes son peque˜nos, aumenta la capacidad de generalizaci´on, mientras que los lotes m´as grandes aceleran el entrenamiento. Se suele experimentar o hacer validaci´on cruzada.\\n\\n8',\n",
    " '3.3 LSTM\\n\\nImagen de flujo de informaci´on en una LSTM:\\n\\nCompuerta de Compuerta de ‘Memoria a largo plazo /* datos actuales Compuerta de olvido ‘Memoria a corto plazo con datos actuales Q Xt \\\\ anes de entrada\\n\\nFigure 1: Flujo de informaci´on en una LSTM\\n\\n3.4 Flujo de informaci´on en las LSTM\\n\\nA lo largo de cada paso de tiempo de una secuencia, las LSTM pasan por una serie de actualizaciones durante las cuales las puertas deciden qu´e informaci´on se mantiene, se olvida o se utiliza de una forma m´as eficiente que las RNNs cl´asicas.\\n\\nEsta arquitectura se puede usar tanto en la traducci´on de una oraci´on, ya que puede utilizar el contexto para traducir palabras que puedan tener diferentes significados seg´un el contexto, como en la predicci´on de series temporales, ya que puede recordar informaci´on relevante a lo largo de la secuencia. Tambi´en son ´utiles para modelar dependencias a largo plazo.\\n\\n4 Comparaci´on de LSTM con RNN est´andar y GRU\\n\\n4.1 Comparaci´on entre RNN, LSTM y GRU\\n\\nPara solucionar el problema de las RNN est´andar de recordar informaci´on a largo plazo, se han desar- rollado dos arquitecturas de RNN: las LSTM y las GRU (Gated Recurrent Unit).\\n\\nEn una RNN tradicional, cada neurona toma como entrada tanto la entrada actual de la secuencia como el estado oculto de la iteraci´on anterior.\\n\\n4.2 Ventajas LSTM\\n\\nFueron introducidas por Hochreiter y Schmidhuber en 1997. Son una evoluci´on de las RNNs, dise˜nadas para resolver el problema del desvanecimiento del gradiente. Ventajas:\\n\\n5\\n\\n• Capacidad para manejar dependencias a largo plazo: Gracias a la c´elula de memoria y las puertas, las LSTM pueden recordar informaci´on durante per´ıodos prolongados, lo que las hace ideales para tareas que requieren un contexto a largo plazo.\\n\\n• Flexibilidad: Deciden de forma din´amica qu´e informaci´on mantener y qu´e informaci´on olvidar, lo que les permite adaptarse a diferentes tipos de datos y tareas.\\n\\n• Mitigaci´on del desvanecimiento del gradiente: Evitan que, durante el Backpropagation, los gradientes disminuyan progresivamente desvanci´endose.',\n",
    " '3.2 Componentes clave de las LSTM\\n\\nLos componentes clave de las LSTM que las distinguen de las RNN est´andar son:\\n\\n• C´elula de memoria: Es el componente central de las LSTM y, a diferencia de las RNN tradi- cionales, que no tienen memoria, las LSTM est´an dise˜nadas para recordar informaci´on durante largos pasos de tiempo. La celda de memoria act´ua como un contenedor con informaci´on que puede actualizarse, olvidarse o utilizarse seg´un sea necesario. El estado de la c´elula de memoria es lo que permite a las LSTM conservar el contexto relevante durante per´ıodos m´as largos. El valor de la celda puede cambiar seg´un la red crea conveniente, lo que permite que la LSTM decida qu´e informaci´on recordar y qu´e informaci´on olvidar.\\n\\n• Puertas en LSTM: son mecanismos que controlan de manera precisa c´omo fluye la informaci´on a trav´es de la c´elula de memoria.\\n\\n– Puerta de olvido: Es un componente muy importante en la arquitectura de las LSTM ya que decide cu´anta informaci´on previa debe descartarse. Es importante para evitar que la c´elula de memoria almacene informaci´on irrelevante. Matem´aticamente, la puerta de olvido se define mediante una funci´on sigmoide f , el estado oculto anterior ht−1 y el input actual xt. Produce un valor entre 0 y 1 que determina qu´e fracci´on de la memoria anterior se mantendr´a. Si es cercano a 0, la c´elula de memoria se olvida de gran parte de la informaci´on anterior, mientras que si es cercano a 1, se mantiene casi toda la informaci´on para utilizarla en el futuro.\\n\\nft = σ(wf[ht−1,xt] + bf) (6)\\n\\nNota: f se debe a forget\\n\\n– Puerta de entrada: Es responsable de decidir qu´e nueva informaci´on debe almacenarse en la celda de memoria. Act´ua en conjunto con otro valor denominado nuevo candidato de memoria o ˜ct, que es calculada mediante una funci´on tangente hiperb´olica para generar una posible actualizaci´on de la memoria. Usa ˜ct para determinar qu´e informaci´on se va a a˜nadir a la celda de memoria. Matem´aticamente, la puerta de entrada se define mediante una funci´on sigmoide σ, el estado oculto anterior ht−1 y el input actual xt. Produce un valor entre 0 y 1, al igual que la puerta de olvido, que controla la magnitud con la que la nueva informaci´on actualiza la memoria. La combinaci´on de la puerta de entrada y con el candidato de memoria permite a las LSTM incorporar informaci´on nueva ajust´andose din´amicamente a las condiciones cambiantes de la secuencia.\\n\\nit = σ(wi[ht−1,xt] + bi) (7)\\n\\nNota: i se debe a input\\n\\n˜ct = tanh(wc[ht−1,xt] + bc) (8)\\n\\n4\\n\\n– Puerta de salida: Es la encargada de determinar qu´e parte de la informaci´on contenida dentro de la c´elula debe usarse para generar la salida de la LSTM en el instante temporal t. Esto es importante ya que no toda la informaci´on almacenada en la celda de memoria es relevante para la salida. Act´ua como un filtro que decide qu´e informaci´on debe pasarse al estado oculto ht y, por lo tanto, a la siguiente iteraci´on.\\n\\not = σ(wo[ht−1,xt] + bo) (9)\\n\\nNota: o se debe a output\\n\\nht = ot · tanh(ct) (10)\\n\\nAs´ı se obtiene el estado oculto ht, a partir de la celda de memoria ct y la puerta de salida ot.'],\n",
    "    \"\"\"Las limitaciones de las LSTM son:\n",
    "\n",
    "Complejidad y coste computacional: Las LSTM son más complejas debido a las puertas y la célula de memoria, lo que aumenta el coste computacional y el tiempo de entrenamiento. Por lo tanto, en situaciones con recursos limitados o donde se necesita un entrenamiento rápido, las LSTM pueden no ser la mejor opción. (Sección 6.1, página 1)\n",
    "Problemas con secuencias extremadamente largas: Aunque las LSTM son mejores que las RNNs en el manejo de secuencias largas, aún pueden tener problemas con el desvanecimiento del gradiente a muy largo plazo, lo que les lleva a perder información relevante. (Sección 6.1, página 1)\n",
    "Riesgo de sobreajuste: Este riesgo es considerable en conjuntos de datos muy pequeños, lo que puede hacer que la red retenga patrones específicos sin generalizar bien a nuevos datos. (Sección 6.1, página 1)\"\"\",\n",
    "    \"\"\"Limitaciones de las LSTM\n",
    "• Complejidad y coste computacional: las puertas y la célula de memoria añaden complejidad\n",
    "a la red, además de aumentar el coste computacional y el tiempo de entrenamiento. Así que en\n",
    "casos en los que haya recursos limitados o se necesite un entrenamiento rápido, las LSTM pueden\n",
    "no ser la mejor opción.\n",
    "• Problemas con secuencias extremadamente largas: aunque son bastante mejores que las\n",
    "RNNs en escenarios donde las secuencias son extremadamente largas, pueden seguir teniendo problemas\n",
    "con el desvanecimiento del gradiente a muy largo plazo perdiendo información relevante.\n",
    "• Riesgo de sobreajuste: este riesgo es considerable en conjuntos de datos muy pequeños, por lo\n",
    "que retiene patrones espec´íficos sin generalizar bien a nuevos datos.\"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76671981",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[len(df)] = [\n",
    "    \"Respecto al ajuste de hiperparámetros, ¿qué mejoras se podrían hacer en las LSTM?\",\n",
    "    ['6 Limitaciones y mejores pr´acticas de LSTM\\n\\n6.1 Limitaciones de las LSTM\\n\\n• Complejidad y coste computacional: las puertas y la c´elula de memoria a˜naden complejidad a la red, adem´as de aumentar el coste computacional y el tiempo de entrenamiento. As´ı que en casos en los que haya recursos limitados o se necesite un entrenamiento r´apido, las LSTM pueden no ser la mejor opci´on.\\n\\n• Problemas con secuencias extremadamente largas: aunque son bastante mejores que las RNNs en escenarios donde las secuencias son extremadamente largas, pueden seguir teniendo prob- lemas con el desvanecimiento del gradiente a muy largo plazo perdiendo informaci´on relevante.\\n\\n• Riesgo de sobreajuste: este riesgo es considerable en conjuntos de datos muy peque˜nos, por lo que retiene patrones espec´ıficos sin generalizar bien a nuevos datos.\\n\\n6.2 Mejores pr´acticas de LSTM\\n\\n• Normalizaci´on de datos: Una de las primeras pr´acticas a implementar al entrenar una LSTM. Ayuda a mejorar la convergencia del modelo, haci´endolo m´as eficiente y evitando que el modelo quede atrapado en m´ınimos locales durante el proceso de optimizaci´on. En el caso de secuencias temporales, esto puede implicar la normalizaci´on de las features (normalmente de 0 a 1) y, al reducir la escala de las entradas, se facilita que las LSTM aprendan patrones en los datos y mantengan un mejor equilibrio en los gradientes propagados durante el entrenamiento. Sin una normalizaci´on adecuada, los gradientes pueden ser muy grandes o muy peque˜nos, lo que puede dificultar el aprendizaje de la red.\\n\\n• Uso de regulaci´on para evitar sobreajuste:\\n\\n– Dropout: Consiste en apagar aleatoriamente una fracci´on de las neuronas en cada paso del entrenamiento. Esto obliga al modelo a ser m´as robusto, porque no va a depender excesiva- mente de caracter´ısticas muy espec´ıficas.\\n\\n∗ Dropout en las conexiones recurrentes: apaga neuronas en las conexiones que hay entre capas LSTM, evitando que el modelo memorice patrones temporales innecesarios.\\n\\n∗ Dropout en las capas de salida: Se puede aplicar tras las capas Fully Connected del final de la red. Aunque es m´as com´un aplicarlo en las capas intermedias.\\n\\n– L2 regularization: esta t´ecnica penaliza los pesos muy altos del modelo haciendo que los par´ametros se mantengan bajo control y ayuda a evitar el overfitting.\\n\\nExperimentaci´on con capas bidireccionales: Las capas bidireccionales permiten al modelo\\n\\nconsiderar tanto el contexto anterior como posterior en una secuencia, lo cual es ´util en tareas donde el contexto completo mejora la precisi´on, como en el procesamiento de lenguaje natural.\\n\\nUso de modelos apilados (stacked LSTM): para mejorar la capacidad de las LSTM, se pueden\\n\\napilar m´ultiples capas LSTM. Esto permite que el modelo aprenda representaciones m´as complejas y capture mejor caracter´ısticas de datos secuenciales. Cada capa LSTM a˜nadida puede ayudar a procesar los datos de una forma m´as compleja o abstracta. Esta pr´actica aumenta la complejidad, por tanto, tambi´en incrementa los tiempos de entrenamiento y el riesgo de sobreajuste.\\n\\nAjuste de hiperpar´ametros:\\n\\n– Tama˜no del n´umero de unidades LSTM: es decir, cu´antas unidades va a tener la LSTM lo que controla la cantidad de memoria y la capacidad de aprendizaje de la red. Captura patrones m´as complejos. Aunque aumenta el riesgo de sobreajuste.\\n\\n– Tasa de aprendizaje: debe ser equilibrada, lo suficientemente peque˜na para garantizar que el modelo aprenda de manera gradual y efectiva, pero no tanto como para hacer ineficiente el proceso de entrenamiento.\\n\\n– Batch size: si los lotes son peque˜nos, aumenta la capacidad de generalizaci´on, mientras que los lotes m´as grandes aceleran el entrenamiento. Se suele experimentar o hacer validaci´on cruzada.\\n\\n8',\n",
    " '3.3 LSTM\\n\\nImagen de flujo de informaci´on en una LSTM:\\n\\nCompuerta de Compuerta de ‘Memoria a largo plazo /* datos actuales Compuerta de olvido ‘Memoria a corto plazo con datos actuales Q Xt \\\\ anes de entrada\\n\\nFigure 1: Flujo de informaci´on en una LSTM\\n\\n3.4 Flujo de informaci´on en las LSTM\\n\\nA lo largo de cada paso de tiempo de una secuencia, las LSTM pasan por una serie de actualizaciones durante las cuales las puertas deciden qu´e informaci´on se mantiene, se olvida o se utiliza de una forma m´as eficiente que las RNNs cl´asicas.\\n\\nEsta arquitectura se puede usar tanto en la traducci´on de una oraci´on, ya que puede utilizar el contexto para traducir palabras que puedan tener diferentes significados seg´un el contexto, como en la predicci´on de series temporales, ya que puede recordar informaci´on relevante a lo largo de la secuencia. Tambi´en son ´utiles para modelar dependencias a largo plazo.\\n\\n4 Comparaci´on de LSTM con RNN est´andar y GRU\\n\\n4.1 Comparaci´on entre RNN, LSTM y GRU\\n\\nPara solucionar el problema de las RNN est´andar de recordar informaci´on a largo plazo, se han desar- rollado dos arquitecturas de RNN: las LSTM y las GRU (Gated Recurrent Unit).\\n\\nEn una RNN tradicional, cada neurona toma como entrada tanto la entrada actual de la secuencia como el estado oculto de la iteraci´on anterior.\\n\\n4.2 Ventajas LSTM\\n\\nFueron introducidas por Hochreiter y Schmidhuber en 1997. Son una evoluci´on de las RNNs, dise˜nadas para resolver el problema del desvanecimiento del gradiente. Ventajas:\\n\\n5\\n\\n• Capacidad para manejar dependencias a largo plazo: Gracias a la c´elula de memoria y las puertas, las LSTM pueden recordar informaci´on durante per´ıodos prolongados, lo que las hace ideales para tareas que requieren un contexto a largo plazo.\\n\\n• Flexibilidad: Deciden de forma din´amica qu´e informaci´on mantener y qu´e informaci´on olvidar, lo que les permite adaptarse a diferentes tipos de datos y tareas.\\n\\n• Mitigaci´on del desvanecimiento del gradiente: Evitan que, durante el Backpropagation, los gradientes disminuyan progresivamente desvanci´endose.',\n",
    " '3.2 Componentes clave de las LSTM\\n\\nLos componentes clave de las LSTM que las distinguen de las RNN est´andar son:\\n\\n• C´elula de memoria: Es el componente central de las LSTM y, a diferencia de las RNN tradi- cionales, que no tienen memoria, las LSTM est´an dise˜nadas para recordar informaci´on durante largos pasos de tiempo. La celda de memoria act´ua como un contenedor con informaci´on que puede actualizarse, olvidarse o utilizarse seg´un sea necesario. El estado de la c´elula de memoria es lo que permite a las LSTM conservar el contexto relevante durante per´ıodos m´as largos. El valor de la celda puede cambiar seg´un la red crea conveniente, lo que permite que la LSTM decida qu´e informaci´on recordar y qu´e informaci´on olvidar.\\n\\n• Puertas en LSTM: son mecanismos que controlan de manera precisa c´omo fluye la informaci´on a trav´es de la c´elula de memoria.\\n\\n– Puerta de olvido: Es un componente muy importante en la arquitectura de las LSTM ya que decide cu´anta informaci´on previa debe descartarse. Es importante para evitar que la c´elula de memoria almacene informaci´on irrelevante. Matem´aticamente, la puerta de olvido se define mediante una funci´on sigmoide f , el estado oculto anterior ht−1 y el input actual xt. Produce un valor entre 0 y 1 que determina qu´e fracci´on de la memoria anterior se mantendr´a. Si es cercano a 0, la c´elula de memoria se olvida de gran parte de la informaci´on anterior, mientras que si es cercano a 1, se mantiene casi toda la informaci´on para utilizarla en el futuro.\\n\\nft = σ(wf[ht−1,xt] + bf) (6)\\n\\nNota: f se debe a forget\\n\\n– Puerta de entrada: Es responsable de decidir qu´e nueva informaci´on debe almacenarse en la celda de memoria. Act´ua en conjunto con otro valor denominado nuevo candidato de memoria o ˜ct, que es calculada mediante una funci´on tangente hiperb´olica para generar una posible actualizaci´on de la memoria. Usa ˜ct para determinar qu´e informaci´on se va a a˜nadir a la celda de memoria. Matem´aticamente, la puerta de entrada se define mediante una funci´on sigmoide σ, el estado oculto anterior ht−1 y el input actual xt. Produce un valor entre 0 y 1, al igual que la puerta de olvido, que controla la magnitud con la que la nueva informaci´on actualiza la memoria. La combinaci´on de la puerta de entrada y con el candidato de memoria permite a las LSTM incorporar informaci´on nueva ajust´andose din´amicamente a las condiciones cambiantes de la secuencia.\\n\\nit = σ(wi[ht−1,xt] + bi) (7)\\n\\nNota: i se debe a input\\n\\n˜ct = tanh(wc[ht−1,xt] + bc) (8)\\n\\n4\\n\\n– Puerta de salida: Es la encargada de determinar qu´e parte de la informaci´on contenida dentro de la c´elula debe usarse para generar la salida de la LSTM en el instante temporal t. Esto es importante ya que no toda la informaci´on almacenada en la celda de memoria es relevante para la salida. Act´ua como un filtro que decide qu´e informaci´on debe pasarse al estado oculto ht y, por lo tanto, a la siguiente iteraci´on.\\n\\not = σ(wo[ht−1,xt] + bo) (9)\\n\\nNota: o se debe a output\\n\\nht = ot · tanh(ct) (10)\\n\\nAs´ı se obtiene el estado oculto ht, a partir de la celda de memoria ct y la puerta de salida ot.'],\n",
    "    \"\"\"Respecto al ajuste de hiperparámetros en LSTM, se pueden considerar las siguientes mejoras:\n",
    "\n",
    "Tamaño del número de unidades LSTM: Ajustar cuántas unidades tendrá la LSTM, ya que esto controla la cantidad de memoria y la capacidad de aprendizaje de la red. Un mayor número de unidades puede capturar patrones más complejos, aunque también aumenta el riesgo de sobreajuste. (Sección 6.2 Mejores prácticas de LSTM).\n",
    "Tasa de aprendizaje: Es importante que la tasa de aprendizaje sea equilibrada. Debe ser lo suficientemente pequeña para asegurar que el modelo aprenda de manera gradual y efectiva, pero no tan pequeña que haga ineficiente el proceso de entrenamiento. (Sección 6.2 Mejores prácticas de LSTM).\n",
    "Tamaño del batch: Experiementar con el tamaño de los lotes de datos. Tamaños de lotes más pequeños pueden aumentar la capacidad de generalización del modelo, mientras que tamaños más grandes pueden acelerar el entrenamiento. Se recomienda realizar validación cruzada para encontrar el tamaño óptimo. (Sección 6.2 Mejores prácticas de LSTM).\n",
    "Estas prácticas pueden ayudar a optimizar el rendimiento y la efectividad del modelo LSTM.\"\"\",\n",
    "    \"\"\"Ajuste de hiperparámetros:\n",
    "- Tamaño del número de unidades LSTM: es decir, cuántas unidades va a tener la LSTM\n",
    "lo que controla la cantidad de memoria y la capacidad de aprendizaje de la red. Captura\n",
    "patrones más complejos. Aunque aumenta el riesgo de sobreajuste.\n",
    "- Tasa de aprendizaje: debe ser equilibrada, lo suficientemente pequeña para garantizar que\n",
    "el modelo aprenda de manera gradual y efectiva, pero no tanto como para hacer ineficiente el\n",
    "proceso de entrenamiento.\n",
    "- Batch size: si los lotes son pequeños, aumenta la capacidad de generalización, mientras que\n",
    "los lotes más grandes aceleran el entrenamiento. Se suele experimentar o hacer validación\n",
    "cruzada.\"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8a4685",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[len(df)] = [\n",
    "    \"¿Hay alguna técnica para reducir sobreajuste en las LSTM?\",\n",
    "    ['6 Limitaciones y mejores pr´acticas de LSTM\\n\\n6.1 Limitaciones de las LSTM\\n\\n• Complejidad y coste computacional: las puertas y la c´elula de memoria a˜naden complejidad a la red, adem´as de aumentar el coste computacional y el tiempo de entrenamiento. As´ı que en casos en los que haya recursos limitados o se necesite un entrenamiento r´apido, las LSTM pueden no ser la mejor opci´on.\\n\\n• Problemas con secuencias extremadamente largas: aunque son bastante mejores que las RNNs en escenarios donde las secuencias son extremadamente largas, pueden seguir teniendo prob- lemas con el desvanecimiento del gradiente a muy largo plazo perdiendo informaci´on relevante.\\n\\n• Riesgo de sobreajuste: este riesgo es considerable en conjuntos de datos muy peque˜nos, por lo que retiene patrones espec´ıficos sin generalizar bien a nuevos datos.\\n\\n6.2 Mejores pr´acticas de LSTM\\n\\n• Normalizaci´on de datos: Una de las primeras pr´acticas a implementar al entrenar una LSTM. Ayuda a mejorar la convergencia del modelo, haci´endolo m´as eficiente y evitando que el modelo quede atrapado en m´ınimos locales durante el proceso de optimizaci´on. En el caso de secuencias temporales, esto puede implicar la normalizaci´on de las features (normalmente de 0 a 1) y, al reducir la escala de las entradas, se facilita que las LSTM aprendan patrones en los datos y mantengan un mejor equilibrio en los gradientes propagados durante el entrenamiento. Sin una normalizaci´on adecuada, los gradientes pueden ser muy grandes o muy peque˜nos, lo que puede dificultar el aprendizaje de la red.\\n\\n• Uso de regulaci´on para evitar sobreajuste:\\n\\n– Dropout: Consiste en apagar aleatoriamente una fracci´on de las neuronas en cada paso del entrenamiento. Esto obliga al modelo a ser m´as robusto, porque no va a depender excesiva- mente de caracter´ısticas muy espec´ıficas.\\n\\n∗ Dropout en las conexiones recurrentes: apaga neuronas en las conexiones que hay entre capas LSTM, evitando que el modelo memorice patrones temporales innecesarios.\\n\\n∗ Dropout en las capas de salida: Se puede aplicar tras las capas Fully Connected del final de la red. Aunque es m´as com´un aplicarlo en las capas intermedias.\\n\\n– L2 regularization: esta t´ecnica penaliza los pesos muy altos del modelo haciendo que los par´ametros se mantengan bajo control y ayuda a evitar el overfitting.\\n\\nExperimentaci´on con capas bidireccionales: Las capas bidireccionales permiten al modelo\\n\\nconsiderar tanto el contexto anterior como posterior en una secuencia, lo cual es ´util en tareas donde el contexto completo mejora la precisi´on, como en el procesamiento de lenguaje natural.\\n\\nUso de modelos apilados (stacked LSTM): para mejorar la capacidad de las LSTM, se pueden\\n\\napilar m´ultiples capas LSTM. Esto permite que el modelo aprenda representaciones m´as complejas y capture mejor caracter´ısticas de datos secuenciales. Cada capa LSTM a˜nadida puede ayudar a procesar los datos de una forma m´as compleja o abstracta. Esta pr´actica aumenta la complejidad, por tanto, tambi´en incrementa los tiempos de entrenamiento y el riesgo de sobreajuste.\\n\\nAjuste de hiperpar´ametros:\\n\\n– Tama˜no del n´umero de unidades LSTM: es decir, cu´antas unidades va a tener la LSTM lo que controla la cantidad de memoria y la capacidad de aprendizaje de la red. Captura patrones m´as complejos. Aunque aumenta el riesgo de sobreajuste.\\n\\n– Tasa de aprendizaje: debe ser equilibrada, lo suficientemente peque˜na para garantizar que el modelo aprenda de manera gradual y efectiva, pero no tanto como para hacer ineficiente el proceso de entrenamiento.\\n\\n– Batch size: si los lotes son peque˜nos, aumenta la capacidad de generalizaci´on, mientras que los lotes m´as grandes aceleran el entrenamiento. Se suele experimentar o hacer validaci´on cruzada.\\n\\n8',\n",
    " '3.3 LSTM\\n\\nImagen de flujo de informaci´on en una LSTM:\\n\\nCompuerta de Compuerta de ‘Memoria a largo plazo /* datos actuales Compuerta de olvido ‘Memoria a corto plazo con datos actuales Q Xt \\\\ anes de entrada\\n\\nFigure 1: Flujo de informaci´on en una LSTM\\n\\n3.4 Flujo de informaci´on en las LSTM\\n\\nA lo largo de cada paso de tiempo de una secuencia, las LSTM pasan por una serie de actualizaciones durante las cuales las puertas deciden qu´e informaci´on se mantiene, se olvida o se utiliza de una forma m´as eficiente que las RNNs cl´asicas.\\n\\nEsta arquitectura se puede usar tanto en la traducci´on de una oraci´on, ya que puede utilizar el contexto para traducir palabras que puedan tener diferentes significados seg´un el contexto, como en la predicci´on de series temporales, ya que puede recordar informaci´on relevante a lo largo de la secuencia. Tambi´en son ´utiles para modelar dependencias a largo plazo.\\n\\n4 Comparaci´on de LSTM con RNN est´andar y GRU\\n\\n4.1 Comparaci´on entre RNN, LSTM y GRU\\n\\nPara solucionar el problema de las RNN est´andar de recordar informaci´on a largo plazo, se han desar- rollado dos arquitecturas de RNN: las LSTM y las GRU (Gated Recurrent Unit).\\n\\nEn una RNN tradicional, cada neurona toma como entrada tanto la entrada actual de la secuencia como el estado oculto de la iteraci´on anterior.\\n\\n4.2 Ventajas LSTM\\n\\nFueron introducidas por Hochreiter y Schmidhuber en 1997. Son una evoluci´on de las RNNs, dise˜nadas para resolver el problema del desvanecimiento del gradiente. Ventajas:\\n\\n5\\n\\n• Capacidad para manejar dependencias a largo plazo: Gracias a la c´elula de memoria y las puertas, las LSTM pueden recordar informaci´on durante per´ıodos prolongados, lo que las hace ideales para tareas que requieren un contexto a largo plazo.\\n\\n• Flexibilidad: Deciden de forma din´amica qu´e informaci´on mantener y qu´e informaci´on olvidar, lo que les permite adaptarse a diferentes tipos de datos y tareas.\\n\\n• Mitigaci´on del desvanecimiento del gradiente: Evitan que, durante el Backpropagation, los gradientes disminuyan progresivamente desvanci´endose.',\n",
    " '1.1 Problemas de las RNN est´andar\\n\\nEstas redes se enfrentan a un problema fundamental, el Desvanecimiento del gradiente. Estas redes se entrenan con un tipo de backpropagation, conocido como Backpropagation Through Time (BPTT). En este algoritmo, el gradiente se propaga hacia atr´as en el tiempo y se ajustan los pesos de la red para minimizar el error. El problema es que, a medida que el error se propaga hacia atr´as en el tiempo, si las secuencias son muy largas, los gradientes se van desvaneciendo, por lo tanto, el impacto del aprendizaje de los primeros pasos de la secuencia es muy peque˜no. Esto hace que las RNN tengan dificultades para aprender de elementos que est´an muy lejos en la secuencia. Cabe destacar que tambi´en puede ocurrir el efecto contrario, la Explosi´on del gradiente, donde los gradientes crecen exponencialmente a medida que se propagan hacia atr´as en el tiempo, por lo que producen inestabilidad en el entrenamiento de la red.\\n\\nEsta es la principal limitaci´on de las RNNs est´andar, que son eficaces a corto plazo. Pero cuando las secuencias son muy largas o la informaci´on relevante est´a muy separada en la secuencia, su rendimiento disminuye significativamente.\\n\\nEste problema ha impulsado que se investigue y se desarrollen nuevas arquitecturas de RNN, como las LSTM y las GRU, que son capaces de recordar informaci´on a largo plazo y evitar el desvanecimiento del gradiente.\\n\\n2 RNN\\n\\nEstas redes permiten usar una variedad m´as amplia de estructuras tanto de entrada como de salida:\\n\\n• 1 a 1: Redes convencionales, un solo input y un solo output.\\n\\n• 1 a N: Por ejemplo: Generaci´on de texto a partir de una imagen.\\n\\nN a 1: Por ejemplo: An´alisis de sentimientos a partir de una rese˜na (de N palabras).\\n\\nN a N: Por ejemplo: Traducci´on de un idioma a otro. Tiene N palabras de entrada y M palabras\\n\\nde salida ya que la traducci´on puede tener m´as o menos palabras que el texto original por lo que t´ecnicamente es N a M.\\n\\nLa funci´on matem´atica que define una RNN es la siguiente:\\n\\nht = fω(ht−1,xt) (1)\\n\\nDonde:\\n\\nht es el estado oculto en el tiempo t.\\n\\nfω es la funci´on de activaci´on. Depende de los pesos ω de la red.\\n\\nht−1 es el estado oculto en el instante anterior, t − 1.\\n\\nxt es el input en el tiempo t.\\n\\n2'],\n",
    "    \"\"\"Hay varias técnicas para reducir el sobreajuste en las LSTM. Según la información proporcionada en la sección 6.2 sobre mejores prácticas de LSTM:\n",
    "\n",
    "Dropout: Esta técnica apaga aleatoriamente una fracción de las neuronas en cada paso del entrenamiento, lo que hace que el modelo sea más robusto, ya que no dependerá excesivamente de características específicas. Se puede aplicar:\n",
    "En las conexiones recurrentes, para evitar que el modelo memorice patrones temporales innecesarios.\n",
    "En las capas de salida, aunque es más común aplicarlo en las capas intermedias. (Sección 6.2).\n",
    "L2 Regularization: Esta técnica penaliza los pesos muy altos del modelo, manteniendo los parámetros bajo control y ayudando a evitar el sobreajuste. (Sección 6.2).\"\"\",\n",
    "    \"\"\"Uso de regulación para evitar sobreajuste:\n",
    "- Dropout: Consiste en apagar aleatoriamente una fracción de las neuronas en cada paso del\n",
    "entrenamiento. Esto obliga al modelo a ser más robusto, porque no va a depender excesivamente\n",
    "de características muy específicas.\n",
    "* Dropout en las conexiones recurrentes: apaga neuronas en las conexiones que hay\n",
    "entre capas LSTM, evitando que el modelo memorice patrones temporales innecesarios.\n",
    "* Dropout en las capas de salida: Se puede aplicar tras las capas Fully Connected del\n",
    "final de la red. Aunque es más común aplicarlo en las capas intermedias.\n",
    "- L2 regularization: esta técnica penaliza los pesos muy altos del modelo haciendo que los\n",
    "parámetros se mantengan bajo control y ayuda a evitar el overfitting.\"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d43755d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[len(df)] = [\n",
    "    \"Dame 4 casos de uso de las LSTM\",\n",
    "    ['6 Limitaciones y mejores pr´acticas de LSTM\\n\\n6.1 Limitaciones de las LSTM\\n\\n• Complejidad y coste computacional: las puertas y la c´elula de memoria a˜naden complejidad a la red, adem´as de aumentar el coste computacional y el tiempo de entrenamiento. As´ı que en casos en los que haya recursos limitados o se necesite un entrenamiento r´apido, las LSTM pueden no ser la mejor opci´on.\\n\\n• Problemas con secuencias extremadamente largas: aunque son bastante mejores que las RNNs en escenarios donde las secuencias son extremadamente largas, pueden seguir teniendo prob- lemas con el desvanecimiento del gradiente a muy largo plazo perdiendo informaci´on relevante.\\n\\n• Riesgo de sobreajuste: este riesgo es considerable en conjuntos de datos muy peque˜nos, por lo que retiene patrones espec´ıficos sin generalizar bien a nuevos datos.\\n\\n6.2 Mejores pr´acticas de LSTM\\n\\n• Normalizaci´on de datos: Una de las primeras pr´acticas a implementar al entrenar una LSTM. Ayuda a mejorar la convergencia del modelo, haci´endolo m´as eficiente y evitando que el modelo quede atrapado en m´ınimos locales durante el proceso de optimizaci´on. En el caso de secuencias temporales, esto puede implicar la normalizaci´on de las features (normalmente de 0 a 1) y, al reducir la escala de las entradas, se facilita que las LSTM aprendan patrones en los datos y mantengan un mejor equilibrio en los gradientes propagados durante el entrenamiento. Sin una normalizaci´on adecuada, los gradientes pueden ser muy grandes o muy peque˜nos, lo que puede dificultar el aprendizaje de la red.\\n\\n• Uso de regulaci´on para evitar sobreajuste:\\n\\n– Dropout: Consiste en apagar aleatoriamente una fracci´on de las neuronas en cada paso del entrenamiento. Esto obliga al modelo a ser m´as robusto, porque no va a depender excesiva- mente de caracter´ısticas muy espec´ıficas.\\n\\n∗ Dropout en las conexiones recurrentes: apaga neuronas en las conexiones que hay entre capas LSTM, evitando que el modelo memorice patrones temporales innecesarios.\\n\\n∗ Dropout en las capas de salida: Se puede aplicar tras las capas Fully Connected del final de la red. Aunque es m´as com´un aplicarlo en las capas intermedias.\\n\\n– L2 regularization: esta t´ecnica penaliza los pesos muy altos del modelo haciendo que los par´ametros se mantengan bajo control y ayuda a evitar el overfitting.\\n\\nExperimentaci´on con capas bidireccionales: Las capas bidireccionales permiten al modelo\\n\\nconsiderar tanto el contexto anterior como posterior en una secuencia, lo cual es ´util en tareas donde el contexto completo mejora la precisi´on, como en el procesamiento de lenguaje natural.\\n\\nUso de modelos apilados (stacked LSTM): para mejorar la capacidad de las LSTM, se pueden\\n\\napilar m´ultiples capas LSTM. Esto permite que el modelo aprenda representaciones m´as complejas y capture mejor caracter´ısticas de datos secuenciales. Cada capa LSTM a˜nadida puede ayudar a procesar los datos de una forma m´as compleja o abstracta. Esta pr´actica aumenta la complejidad, por tanto, tambi´en incrementa los tiempos de entrenamiento y el riesgo de sobreajuste.\\n\\nAjuste de hiperpar´ametros:\\n\\n– Tama˜no del n´umero de unidades LSTM: es decir, cu´antas unidades va a tener la LSTM lo que controla la cantidad de memoria y la capacidad de aprendizaje de la red. Captura patrones m´as complejos. Aunque aumenta el riesgo de sobreajuste.\\n\\n– Tasa de aprendizaje: debe ser equilibrada, lo suficientemente peque˜na para garantizar que el modelo aprenda de manera gradual y efectiva, pero no tanto como para hacer ineficiente el proceso de entrenamiento.\\n\\n– Batch size: si los lotes son peque˜nos, aumenta la capacidad de generalizaci´on, mientras que los lotes m´as grandes aceleran el entrenamiento. Se suele experimentar o hacer validaci´on cruzada.\\n\\n8',\n",
    " '3.3 LSTM\\n\\nImagen de flujo de informaci´on en una LSTM:\\n\\nCompuerta de Compuerta de ‘Memoria a largo plazo /* datos actuales Compuerta de olvido ‘Memoria a corto plazo con datos actuales Q Xt \\\\ anes de entrada\\n\\nFigure 1: Flujo de informaci´on en una LSTM\\n\\n3.4 Flujo de informaci´on en las LSTM\\n\\nA lo largo de cada paso de tiempo de una secuencia, las LSTM pasan por una serie de actualizaciones durante las cuales las puertas deciden qu´e informaci´on se mantiene, se olvida o se utiliza de una forma m´as eficiente que las RNNs cl´asicas.\\n\\nEsta arquitectura se puede usar tanto en la traducci´on de una oraci´on, ya que puede utilizar el contexto para traducir palabras que puedan tener diferentes significados seg´un el contexto, como en la predicci´on de series temporales, ya que puede recordar informaci´on relevante a lo largo de la secuencia. Tambi´en son ´utiles para modelar dependencias a largo plazo.\\n\\n4 Comparaci´on de LSTM con RNN est´andar y GRU\\n\\n4.1 Comparaci´on entre RNN, LSTM y GRU\\n\\nPara solucionar el problema de las RNN est´andar de recordar informaci´on a largo plazo, se han desar- rollado dos arquitecturas de RNN: las LSTM y las GRU (Gated Recurrent Unit).\\n\\nEn una RNN tradicional, cada neurona toma como entrada tanto la entrada actual de la secuencia como el estado oculto de la iteraci´on anterior.\\n\\n4.2 Ventajas LSTM\\n\\nFueron introducidas por Hochreiter y Schmidhuber en 1997. Son una evoluci´on de las RNNs, dise˜nadas para resolver el problema del desvanecimiento del gradiente. Ventajas:\\n\\n5\\n\\n• Capacidad para manejar dependencias a largo plazo: Gracias a la c´elula de memoria y las puertas, las LSTM pueden recordar informaci´on durante per´ıodos prolongados, lo que las hace ideales para tareas que requieren un contexto a largo plazo.\\n\\n• Flexibilidad: Deciden de forma din´amica qu´e informaci´on mantener y qu´e informaci´on olvidar, lo que les permite adaptarse a diferentes tipos de datos y tareas.\\n\\n• Mitigaci´on del desvanecimiento del gradiente: Evitan que, durante el Backpropagation, los gradientes disminuyan progresivamente desvanci´endose.',\n",
    " 'LSTM\\n\\nContents\\n\\n1.1 Problemas de las RNN est´andar . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.1 Vanilla RNN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.2 Flujo de RNN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.3 Ejemplo RNN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.4 Aplicaci´on sobre im´agenes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.1 LSTM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2 Componentes clave de las LSTM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3 LSTM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.4 Flujo de informaci´on en las LSTM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 Comparaci´on de LSTM con RNN est´andar y GRU 4.1 Comparaci´on entre RNN, LSTM y GRU . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2 Ventajas LSTM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.3 GRU . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.4 Comparaci´on entre LSTM y GRU . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.5 Resumen comparativo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.1 Casos de uso de las LSTM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 Limitaciones y mejores pr´acticas de LSTM 6.1 Limitaciones de las LSTM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.2 Mejores pr´acticas de LSTM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 2 2 3 3 3 3 4 4 4 5 5 5 5 5 6 6 7 7 7 8 8 8\\n\\n1 Introducci´on a RNN\\n\\n2 RNN\\n\\n3 Introducci´on a LSTM\\n\\n5 ¿Cu´ando usar LSTM?\\n\\n1\\n\\n1 Introducci´on a RNN\\n\\nLas Redes Neuronales Recurrentes son un tipo espec´ıfico de arquitectura de red neuronal que est´a dise˜nado para manejar datos secuenciales, lo que significa que el orden de los datos es muy impor- tante. Este tipo de red es fundamental en tareas donde la informaci´on anterior debe ser recordada o procesada junto a la informaci´on actual. A lo largo de la historia de la IA, las RNN han sido muy impor- tantes a la hora de resolver problemas relacionados con el an´alisis de frecuencias como NLP, predicciones de series temporales, transcripci´on de voz a texto o modelado de v´ıdeo y audio.\\n\\nEn las redes convencionales como las feedforward, la salida solo depende del input actual. Sin embargo, en las RNN, la salida depende tanto del input actual como de los inputs anteriores al introducir el concepto de memoria. Esta capacidad de recordar informaci´on previa es lo que hace que las RNN sean tan poderosas en la resoluci´on de problemas secuenciales.'],\n",
    "    \"\"\"Los casos de uso de las LSTM incluyen:\n",
    "\n",
    "Traducción de oraciones: Las LSTM pueden utilizar el contexto para traducir palabras que tienen diferentes significados según el contexto.\n",
    "Predicción de series temporales: Son útiles para recordar información relevante a lo largo de la secuencia.\n",
    "Modelar dependencias a largo plazo: Su diseño les permite manejar dependencias en datos secuenciales, lo que es crucial en muchas aplicaciones de análisis de datos.\n",
    "Procesamiento de lenguaje natural: Consisten en tareas como la generación de texto y análisis de sentimientos, donde el contexto y la secuencia son importantes.\n",
    "Esta información se puede encontrar en la sección 5.1 Casos de uso de las LSTM en la página 5.\"\"\",\n",
    "    \"\"\"Casos de uso de las LSTM\n",
    "• Dependencias temporales a largo plazo: uno de los puntos más fuertes de las LSTM es su\n",
    "capacidad de recordar información en largos periodos de tiempo.\n",
    "- Traducción automática: ya que para entender el significado de una palabra o frase es importante\n",
    "tener en cuenta el contexto.\n",
    "- Generación de textos: ya que para mantener el estilo, la consistencia y la coherencia del texto\n",
    "es importante tener en cuenta lo generado anteriormente.\n",
    "- Reconocimiento de voz: ya que es capaz de identificar patrones pronunciados anteriormente\n",
    "y relacionarlos con el contexto actual.\n",
    "• Problemas con alta complejidad temporal: cuando los datos secuenciales tienen relaciones\n",
    "temporales complejas, las LSTM pueden ser muy útiles ya que, gracias a su célula de memoria y sus\n",
    "puertas, pueden seleccionar qué información es relevante y cuál no, ajustándose así a la complejidad\n",
    "de los datos.\n",
    "- Predicción de series temporales: como acciones o el precio de la energía que vienen influenciados\n",
    "por factores actuales pero también por el estado del mercado en el pasado.\n",
    "- Control robótico o de sistemas:\"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e6c01b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregunta</th>\n",
       "      <th>Contexto</th>\n",
       "      <th>Respuesta_Modelo</th>\n",
       "      <th>Respuesta_Referencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is a attention function?</td>\n",
       "      <td>[3.2 Attention\\n\\nAn attention function can be...</td>\n",
       "      <td>An attention function is a mechanism that maps...</td>\n",
       "      <td>An attention function can be described as mapp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Pregunta  \\\n",
       "0  What is a attention function?   \n",
       "\n",
       "                                            Contexto  \\\n",
       "0  [3.2 Attention\\n\\nAn attention function can be...   \n",
       "\n",
       "                                    Respuesta_Modelo  \\\n",
       "0  An attention function is a mechanism that maps...   \n",
       "\n",
       "                                Respuesta_Referencia  \n",
       "0  An attention function can be described as mapp...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.DataFrame({ \n",
    "    'Pregunta':[\"What is a attention function?\"],\n",
    "    'Contexto': [['3.2 Attention\\n\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n\\n3\\n\\nScaled Dot-Product Attention\\n\\nMulti-Head Attention\\n\\nLinear\\n\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.\\n\\nof the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\\n\\n3.2.1 Scaled Dot-Product Attention\\n\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the √ dk, and apply a softmax function to obtain the weights on the query with all keys, divide each by values.\\n\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices K and V . We compute the matrix of outputs as:\\n\\nAttention(Q,K,V ) = softmax( QKT √ dk )V (1)\\n\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi- plicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor 1√ of . Additive attention computes the compatibility function using a feed-forward network with dk a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.\\n\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk [3]. We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients 4. To counteract this effect, we scale the dot products by 1√ . dk',\n",
    " '3.2.2 Multi-Head Attention\\n\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n\\n‘To illustrate why the dot products get large, assume that the components of q and k are independent random variables with mean 0 and variance 1. Then their dot product, g -k = ves, qiki, has mean 0 and variance dx.\\n\\n4\\n\\noutput values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2.\\n\\nMulti-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\\n\\nMultiHead(Q,K,V ) = Concat(head1,...,headh)W O where headi = Attention(QW Q i ,KW K i ,V W V i )\\n\\nWhere the projections are parameter matrices W Q and W O ∈ Rhdv×dmodel. i ∈ Rdmodel×dk, W K i ∈ Rdmodel×dk, W V i ∈ Rdmodel×dv\\n\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.\\n\\n3.2.3 Applications of Attention in our Model\\n\\nThe Transformer uses multi-head attention in three different ways:\\n\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9].\\n\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.\\n\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to −∞) all values in the input of the softmax which correspond to illegal connections. See Figure 2.']],\n",
    "    'Respuesta_Modelo': [\"\"\"An attention function is a mechanism that maps a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weights assigned to each value are determined by a compatibility function that measures how compatible the query is with the corresponding keys (Section 3.2, page 3).\n",
    "\n",
    "This mechanism enables the model to focus on particular parts of the input when producing an output, allowing for greater flexibility and context-awareness in tasks such as natural language processing.\"\"\"],\n",
    "    'Respuesta_Referencia': [\"\"\"An attention function can be described as mapping a query and a set of key-value pairs to an output,\n",
    "where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n",
    "of the values, where the weight assigned to each value is computed by a compatibility function of the\n",
    "query with the corresponding key.\"\"\"]\n",
    "})\n",
    "\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8da389",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.loc[len(df2)] = [\n",
    "    \"Tell me applications of Attention in our Model\",\n",
    "    ['3.2.2 Multi-Head Attention\\n\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n\\n‘To illustrate why the dot products get large, assume that the components of q and k are independent random variables with mean 0 and variance 1. Then their dot product, g -k = ves, qiki, has mean 0 and variance dx.\\n\\n4\\n\\noutput values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2.\\n\\nMulti-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\\n\\nMultiHead(Q,K,V ) = Concat(head1,...,headh)W O where headi = Attention(QW Q i ,KW K i ,V W V i )\\n\\nWhere the projections are parameter matrices W Q and W O ∈ Rhdv×dmodel. i ∈ Rdmodel×dk, W K i ∈ Rdmodel×dk, W V i ∈ Rdmodel×dv\\n\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.\\n\\n3.2.3 Applications of Attention in our Model\\n\\nThe Transformer uses multi-head attention in three different ways:\\n\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9].\\n\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.\\n\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to −∞) all values in the input of the softmax which correspond to illegal connections. See Figure 2.',\n",
    " '[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In Advances in Neural Information Processing Systems, 2015.\\n\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016.\\n\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with fast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate shift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume 1: Long Papers), pages 434–443. ACL, August 2013.\\n\\n12\\n\\nAttention Visualizations\\n\\n2 i i= RE 3 2 i 2 = = 2c 3 2 £ om % S GBANAAAA fe. Re) [a Q €oe2s ozeseyzses 26e8 TL _ FFREKR8TZESHBOP_,SSESE DSsSSsSESE ~2£FFE€voFEnvnFECRCoecacKRGNESLSESSCEGC -vVvVVVVV HMO KEBOCSRSHLHOD QLSARBYXE FE OH “A ARAAKRAAA “= <2 £ 8 FogesouggsS ss P25 5273 Qvryxapvs\\\\3 es sa 5 Seeneteecorzgrs = Q ogs aaa oO 2 Sere =~ aA o ° 8 aaqaaq0gqaqg o o o 5 > o wWUvvvvVvV Vv —e €E£ © e 2 6 v Do <¢ 8 & |\\n\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for the word ‘making’. Different colors represent different heads. Best viewed in color.\\n\\n13\\n\\n<ped> <ped> UOIUIdO == Aw ul Bulssiw ale « aM = yeum = S| sy ysnf pinoys = uoluldo Aw ul Bulssiw ae ysnf 38q Pinoys uojeojdde Ss}! nq poped 38q JaAou Me] au <ped> <SOa> uojuido Aw ul Bulssiuw oe aM yeum S| SIU} ysnf 3q Pinoys uojeodde Ss}! ynq yoped 3q 4eAeuU meq auL <ped> <SOa> uo|uldo Aw ul Bulssiuw oe eM yeum S| Siu} ysnf 3q Pinoys uoyeoydde si! ynq yoped 3q aul\\n\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top: Full attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5 and 6. Note that the attentions are very sharp for this word.\\n\\n14\\n\\n<ped> <ped> <SOH>\\\\ <SO3> uoluido = uoluido Aw Aw yeyum S| sim pi—f}— 4 -ysn{ | Pinoys «+ pinoys uoeoydde uojeodde si! —=S}! nga A ynq poped pooped aq aq Janou™ J@AoU WIM TIM me) me) oul OUL\\n\\n<ped> <ped> so <0 Uo|UIdO uoluido Aw Aw ul ul Bulssiw Bulssiw ae ale aM am yeuM yeum S| S| sty} # sly -—A - el eq eq pinoys « pinoys uojeoidde ee Ss}! Ss}! nq ee popod a —_ ee eq eq JOAoU JOAoU IW IW rr auL auL\\n\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the sentence. We give two such examples above, from two different heads from the encoder self-attention at layer 5 of 6. The heads clearly learned to perform different tasks.\\n\\n15'],\n",
    "    \"\"\"The Transformer model uses attention in three main ways:\n",
    "\n",
    "Encoder-Decoder Attention: In this layer, queries come from the previous decoder layer, while the memory keys and values come from the encoder's output. This allows each position in the decoder to attend to all positions in the input sequence (Section 3.2.3, page 6).\n",
    "Self-Attention in the Encoder: All keys, values, and queries come from the same source, specifically the output of the previous layer in the encoder. Each position can attend to all positions in the preceding layer of the encoder (Section 3.2.3, page 6).\n",
    "Self-Attention in the Decoder: Similar to the encoder self-attention, but each position in the decoder can only attend to all positions up to and including that position, maintaining the auto-regressive property. This is implemented by masking out future positions (Section 3.2.3, page 6).\"\"\",\n",
    "    \"\"\"The Transformer uses multi-head attention in three different ways:\n",
    "• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\n",
    "and the memory keys and values come from the output of the encoder. This allows every\n",
    "position in the decoder to attend over all positions in the input sequence. This mimics the\n",
    "typical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n",
    "[38, 2, 9].\n",
    "• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\n",
    "and queries come from the same place, in this case, the output of the previous layer in the\n",
    "encoder. Each position in the encoder can attend to all positions in the previous layer of the\n",
    "encoder.\n",
    "• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\n",
    "all positions in the decoder up to and including that position. We need to prevent leftward\n",
    "information flow in the decoder to preserve the auto-regressive property. We implement this\n",
    "inside of scaled dot-product attention by masking out (setting to −∞) all values in the input\n",
    "of the softmax which correspond to illegal connections.\"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41756635",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.loc[len(df2)] = [\n",
    "    \"What is Multi-Head Attention?\",\n",
    "    ['3.2.2 Multi-Head Attention\\n\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n\\n‘To illustrate why the dot products get large, assume that the components of q and k are independent random variables with mean 0 and variance 1. Then their dot product, g -k = ves, qiki, has mean 0 and variance dx.\\n\\n4\\n\\noutput values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2.\\n\\nMulti-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\\n\\nMultiHead(Q,K,V ) = Concat(head1,...,headh)W O where headi = Attention(QW Q i ,KW K i ,V W V i )\\n\\nWhere the projections are parameter matrices W Q and W O ∈ Rhdv×dmodel. i ∈ Rdmodel×dk, W K i ∈ Rdmodel×dk, W V i ∈ Rdmodel×dv\\n\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.\\n\\n3.2.3 Applications of Attention in our Model\\n\\nThe Transformer uses multi-head attention in three different ways:\\n\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9].\\n\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.\\n\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to −∞) all values in the input of the softmax which correspond to illegal connections. See Figure 2.',\n",
    " '[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In Advances in Neural Information Processing Systems, 2015.\\n\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016.\\n\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with fast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate shift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume 1: Long Papers), pages 434–443. ACL, August 2013.\\n\\n12\\n\\nAttention Visualizations\\n\\n2 i i= RE 3 2 i 2 = = 2c 3 2 £ om % S GBANAAAA fe. Re) [a Q €oe2s ozeseyzses 26e8 TL _ FFREKR8TZESHBOP_,SSESE DSsSSsSESE ~2£FFE€voFEnvnFECRCoecacKRGNESLSESSCEGC -vVvVVVVV HMO KEBOCSRSHLHOD QLSARBYXE FE OH “A ARAAKRAAA “= <2 £ 8 FogesouggsS ss P25 5273 Qvryxapvs\\\\3 es sa 5 Seeneteecorzgrs = Q ogs aaa oO 2 Sere =~ aA o ° 8 aaqaaq0gqaqg o o o 5 > o wWUvvvvVvV Vv —e €E£ © e 2 6 v Do <¢ 8 & |\\n\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for the word ‘making’. Different colors represent different heads. Best viewed in color.\\n\\n13\\n\\n<ped> <ped> UOIUIdO == Aw ul Bulssiw ale « aM = yeum = S| sy ysnf pinoys = uoluldo Aw ul Bulssiw ae ysnf 38q Pinoys uojeojdde Ss}! nq poped 38q JaAou Me] au <ped> <SOa> uojuido Aw ul Bulssiuw oe aM yeum S| SIU} ysnf 3q Pinoys uojeodde Ss}! ynq yoped 3q 4eAeuU meq auL <ped> <SOa> uo|uldo Aw ul Bulssiuw oe eM yeum S| Siu} ysnf 3q Pinoys uoyeoydde si! ynq yoped 3q aul\\n\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top: Full attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5 and 6. Note that the attentions are very sharp for this word.\\n\\n14\\n\\n<ped> <ped> <SOH>\\\\ <SO3> uoluido = uoluido Aw Aw yeyum S| sim pi—f}— 4 -ysn{ | Pinoys «+ pinoys uoeoydde uojeodde si! —=S}! nga A ynq poped pooped aq aq Janou™ J@AoU WIM TIM me) me) oul OUL\\n\\n<ped> <ped> so <0 Uo|UIdO uoluido Aw Aw ul ul Bulssiw Bulssiw ae ale aM am yeuM yeum S| S| sty} # sly -—A - el eq eq pinoys « pinoys uojeoidde ee Ss}! Ss}! nq ee popod a —_ ee eq eq JOAoU JOAoU IW IW rr auL auL\\n\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the sentence. We give two such examples above, from two different heads from the encoder self-attention at layer 5 of 6. The heads clearly learned to perform different tasks.\\n\\n15'],\n",
    "    \"\"\"Multi-Head Attention is an attention mechanism that enhances the traditional attention function by linearly projecting the input queries, keys, and values multiple times using different learned linear projections. Specifically, it projects the queries, keys, and values ( h ) times into ( d_k ), ( d_k ), and ( d_v ) dimensions respectively. The attention function is then performed in parallel on each of these projections, resulting in output values that are ( d_v )-dimensional. These outputs are concatenated and projected again to yield the final output values (Section 3.2.2, pages 5-6).\n",
    "\n",
    "This mechanism enables the model to jointly attend to various information from different representation subspaces at different positions, allowing it to capture more complex interactions than a single attention head would (Section 3.2.2, page 6). The standard formulation for multi-head attention is given by:\n",
    "\n",
    "[ \\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h) W_O ]\n",
    "\n",
    "where each head is calculated as:\n",
    "\n",
    "[ \\text{head}_i = \\text{Attention}(Q W_Q^i, K W_K^i, V W_V^i) ]\n",
    "\n",
    "This allows the model to benefit significantly from different representations of the input data through parallelization (Section 3.2.2, page 6).\"\"\",\n",
    "    \"\"\"Instead of performing a single attention function with dmodel-dimensional keys, values and queries,\n",
    "we found it beneficial to linearly project the queries, keys and values h times with different, learned\n",
    "linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\n",
    "queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\n",
    "output values. These are concatenated and once again projected, resulting in the final values, as\n",
    "depicted in Figure 2.\n",
    "Multi-head attention allows the model to jointly attend to information from different representation\n",
    "subspaces at different positions. With a single attention head, averaging inhibits this.\n",
    "[ \\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h) W_O ]\n",
    "where [ \\text{head}_i = \\text{Attention}(Q W_Q^i, K W_K^i, V W_V^i) ]\n",
    "In this work we employ h = 8 parallel attention layers, or heads. For each of these we use\n",
    "dk = dv = dmodel/h = 64. Due to the reduced dimension of each head,\"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc586ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.loc[len(df2)] = [\n",
    "    \"Why self-attention?\",\n",
    "    ['4 Why Self-Attention\\n\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu- tional layers commonly used for mapping one variable-length sequence of symbol representations (x1,...,xn) to another sequence of equal length (z1,...,zn), with xi,zi ∈ Rd, such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata.\\n\\nOne is the total computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required.\\n\\nThe third is the path length between long-range dependencies in the network. Learning long-range dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types.\\n\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence\\n\\n6\\n\\nlength n is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [38] and byte-pair [31] representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in the input sequence centered around the respective output position. This would increase the maximum path length to O(n/r). We plan to investigate this approach further in future work.\\n\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output positions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels, or O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths between any two positions in the network. Convolutional layers are generally more expensive than recurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity considerably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model.\\n\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.',\n",
    " '3.2.2 Multi-Head Attention\\n\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n\\n‘To illustrate why the dot products get large, assume that the components of q and k are independent random variables with mean 0 and variance 1. Then their dot product, g -k = ves, qiki, has mean 0 and variance dx.\\n\\n4\\n\\noutput values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2.\\n\\nMulti-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\\n\\nMultiHead(Q,K,V ) = Concat(head1,...,headh)W O where headi = Attention(QW Q i ,KW K i ,V W V i )\\n\\nWhere the projections are parameter matrices W Q and W O ∈ Rhdv×dmodel. i ∈ Rdmodel×dk, W K i ∈ Rdmodel×dk, W V i ∈ Rdmodel×dv\\n\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.\\n\\n3.2.3 Applications of Attention in our Model\\n\\nThe Transformer uses multi-head attention in three different ways:\\n\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9].\\n\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.\\n\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to −∞) all values in the input of the softmax which correspond to illegal connections. See Figure 2.',\n",
    " '[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In Advances in Neural Information Processing Systems, 2015.\\n\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016.\\n\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with fast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate shift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume 1: Long Papers), pages 434–443. ACL, August 2013.\\n\\n12\\n\\nAttention Visualizations\\n\\n2 i i= RE 3 2 i 2 = = 2c 3 2 £ om % S GBANAAAA fe. Re) [a Q €oe2s ozeseyzses 26e8 TL _ FFREKR8TZESHBOP_,SSESE DSsSSsSESE ~2£FFE€voFEnvnFECRCoecacKRGNESLSESSCEGC -vVvVVVVV HMO KEBOCSRSHLHOD QLSARBYXE FE OH “A ARAAKRAAA “= <2 £ 8 FogesouggsS ss P25 5273 Qvryxapvs\\\\3 es sa 5 Seeneteecorzgrs = Q ogs aaa oO 2 Sere =~ aA o ° 8 aaqaaq0gqaqg o o o 5 > o wWUvvvvVvV Vv —e €E£ © e 2 6 v Do <¢ 8 & |\\n\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for the word ‘making’. Different colors represent different heads. Best viewed in color.\\n\\n13\\n\\n<ped> <ped> UOIUIdO == Aw ul Bulssiw ale « aM = yeum = S| sy ysnf pinoys = uoluldo Aw ul Bulssiw ae ysnf 38q Pinoys uojeojdde Ss}! nq poped 38q JaAou Me] au <ped> <SOa> uojuido Aw ul Bulssiuw oe aM yeum S| SIU} ysnf 3q Pinoys uojeodde Ss}! ynq yoped 3q 4eAeuU meq auL <ped> <SOa> uo|uldo Aw ul Bulssiuw oe eM yeum S| Siu} ysnf 3q Pinoys uoyeoydde si! ynq yoped 3q aul\\n\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top: Full attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5 and 6. Note that the attentions are very sharp for this word.\\n\\n14\\n\\n<ped> <ped> <SOH>\\\\ <SO3> uoluido = uoluido Aw Aw yeyum S| sim pi—f}— 4 -ysn{ | Pinoys «+ pinoys uoeoydde uojeodde si! —=S}! nga A ynq poped pooped aq aq Janou™ J@AoU WIM TIM me) me) oul OUL\\n\\n<ped> <ped> so <0 Uo|UIdO uoluido Aw Aw ul ul Bulssiw Bulssiw ae ale aM am yeuM yeum S| S| sty} # sly -—A - el eq eq pinoys « pinoys uojeoidde ee Ss}! Ss}! nq ee popod a —_ ee eq eq JOAoU JOAoU IW IW rr auL auL\\n\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the sentence. We give two such examples above, from two different heads from the encoder self-attention at layer 5 of 6. The heads clearly learned to perform different tasks.\\n\\n15'],\n",
    "    \"\"\"Self-attention is favored for several reasons compared to recurrent and convolutional layers when mapping variable-length sequences. Here are the main reasons:\n",
    "\n",
    "Computational Complexity: Self-attention layers have a total computational complexity that is generally lower compared to recurrent layers, especially when the sequence length ( n ) is smaller than the representation dimensionality ( d ). Specifically, self-attention connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires ( O(n) ) sequential operations (Section 4, page 6).\n",
    "Parallelization: Self-attention allows for a greater degree of parallelization since it doesn't require sequential processing of input data. This contrasts with recurrent layers, which are inherently sequential and thus limit the possibilities for parallel computation (Section 4, page 6).\n",
    "Learning Long-Range Dependencies: Self-attention helps in effectively learning long-range dependencies by minimizing the path length through which signals traverse in the network. It establishes shorter paths between any combinations of input and output positions, facilitating the learning process (Section 4, page 6).\n",
    "These aspects make self-attention particularly beneficial for sequence transduction tasks, allowing for more efficient and effective models. The information can be found in the section titled \"Why Self-Attention\" on page 6.\"\"\",\n",
    "    \"\"\"In this section we compare various aspects of self-attention layers to the recurrent and convolutional\n",
    "layers commonly used for mapping one variable-length sequence of symbol representations\n",
    "(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈ Rd, such as a hidden\n",
    "layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\n",
    "consider three desiderata.\n",
    "One is the total computational complexity per layer. Another is the amount of computation that can\n",
    "be parallelized, as measured by the minimum number of sequential operations required.\n",
    "The third is the path length between long-range dependencies in the network. Learning long-range\n",
    "dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\n",
    "ability to learn such dependencies is the length of the paths forward and backward signals have to\n",
    "traverse in the network. The shorter these paths between any combination of positions in the input\n",
    "and output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\n",
    "the maximum path length between any two input and output positions in networks composed of the\n",
    "different layer types.\n",
    "As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\n",
    "executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\n",
    "computational complexity, self-attention layers are faster than recurrent layers when the sequence\n",
    "length n is smaller than the representation dimensionality d, which is most often the case with\n",
    "sentence representations used by state-of-the-art models in machine translations, such as word-piece\n",
    "[38] and byte-pair [31] representations. To improve computational performance for tasks involving\n",
    "very long sequences, self-attention could be restricted to considering only a neighborhood of size r in\n",
    "the input sequence centered around the respective output position. This would increase the maximum\n",
    "path length to O(n/r). We plan to investigate this approach further in future work.\n",
    "A single convolutional layer with kernel width k < n does not connect all pairs of input and output\n",
    "positions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\n",
    "or O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths\n",
    "between any two positions in the network. Convolutional layers are generally more expensive than\n",
    "recurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\n",
    "considerably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\n",
    "convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\n",
    "the approach we take in our model.\n",
    "As side benefit, self-attention could yield more interpretable models. We inspect attention distributions\n",
    "from our models and present and discuss examples in the appendix. Not only do individual attention\n",
    "heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\n",
    "and semantic structure of the sentences.\"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbed104",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.loc[len(df2)] = [\n",
    "    \"Each of the layers in our encoder and decoder contains a fully connected feed-forward network? How it works?\",\n",
    "    ['3.3 Position-wise Feed-Forward Networks\\n\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.\\n\\nFFN(x) = max(0,xW1 + b1)W2 + b2 (2)\\n\\nWhile the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality dff = 2048.\\n\\n3.4 Embeddings and Softmax\\n\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor- mation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [30]. In the embedding layers, we multiply those weights by √ dmodel.\\n\\n5\\n\\nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types. n is the sequence length, d is the representation dimension, k is the kernel size of convolutions and r the size of the neighborhood in restricted self-attention.\\n\\nLayer Type Complexity per Layer Sequential Maximum Path Length Operations Self-Attention O(n2 · d) O(1) O(1) Recurrent O(n · d2) O(n) O(n) Convolutional O(k · n · d2) O(1) O(logk(n)) Self-Attention (restricted) O(r · n · d) O(1) O(n/r)\\n\\n3.5 Positional Encoding\\n\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed [9].\\n\\nIn this work, we use sine and cosine functions of different frequencies:\\n\\nPE(pos,2i) = sin(pos/100002i/dmodel) PE(pos,2i+1) = cos(pos/100002i/dmodel)\\n\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of PEpos.\\n\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.',\n",
    " '3 Model Architecture\\n\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35]. Here, the encoder maps an input sequence of symbol representations (x1,...,xn) to a sequence of continuous representations z = (z1,...,zn). Given z, the decoder then generates an output sequence (y1,...,ym) of symbols one element at a time. At each step the model is auto-regressive [10], consuming the previously generated symbols as additional input when generating the next.\\n\\n2\\n\\nOutput Probabilities Add & Norm Feed Forward Add & Norm Multi-Head Attention a, Add & Norm Add & Norm Feed Forward Nx | -+CAgc8 Norm) Add & Norm Masked Multi-Head Multi-Head Attention Attention Se a, ee a, Positional Positional Encoding @ © @ Encoding Input Output Embedding Embedding Inputs Outputs (shifted right)\\n\\nFigure 1: The Transformer - model architecture.\\n\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.\\n\\n3.1 Encoder and Decoder Stacks\\n\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position- wise fully connected feed-forward network. We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512.\\n\\nDecoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.'],\n",
    "    \"\"\"Each layer in the encoder and decoder of the Transformer architecture contains a fully connected feed-forward network (FFN) that operates on each position separately and identically. This feed-forward network consists of two linear transformations with a ReLU activation function between them. The formula describing the feed-forward function is:\n",
    "\n",
    "[ \\text{FFN}(x) = \\max(0, x W_1 + b_1) W_2 + b_2 ]\n",
    "\n",
    "In this function:\n",
    "\n",
    "The linear transformations use weight matrices ( W_1 ) and ( W_2 ) and biases ( b_1 ) and ( b_2 ).\n",
    "The input and output dimensions of these transformations are specified as ( d_{\\text{model}} = 512 ) and the inner-layer dimension as ( d_{ff} = 2048 ).\n",
    "Although the transformations are the same across different positions, they use different parameters from layer to layer.\n",
    "Another way to interpret this feed-forward network is as applying two convolutions with a kernel size of 1 (found in Section 3.3, \"Position-wise Feed-Forward Networks,\" and the corresponding descriptions on page 5).\"\"\",\n",
    "    \"\"\"In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\n",
    "connected feed-forward network, which is applied to each position separately and identically. This\n",
    "consists of two linear transformations with a ReLU activation in between.\n",
    "FFN(x) = max(0,xW1 + b1)W2 + b2 (2)\n",
    "While the linear transformations are the same across different positions, they use different parameters\n",
    "from layer to layer. Another way of describing this is as two convolutions with kernel size 1.\n",
    "The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\n",
    "dff = 2048.\"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac5b48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.loc[len(df2)] = [\n",
    "    \"In what way does the model incorporate the order of the sequence into its computations?\",\n",
    "    ['1 Introduction\\n\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38, 24, 15].\\n\\nRecurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved significant improvements in computational efficiency through factorization tricks [21] and conditional computation [32], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains.\\n\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms are used in conjunction with a recurrent network.\\n\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n\\n2 Background\\n\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions [12]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2.\\n\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\n\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence- aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [34].\\n\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence- aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17, 18] and [9].',\n",
    " '3 Model Architecture\\n\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35]. Here, the encoder maps an input sequence of symbol representations (x1,...,xn) to a sequence of continuous representations z = (z1,...,zn). Given z, the decoder then generates an output sequence (y1,...,ym) of symbols one element at a time. At each step the model is auto-regressive [10], consuming the previously generated symbols as additional input when generating the next.\\n\\n2\\n\\nOutput Probabilities Add & Norm Feed Forward Add & Norm Multi-Head Attention a, Add & Norm Add & Norm Feed Forward Nx | -+CAgc8 Norm) Add & Norm Masked Multi-Head Multi-Head Attention Attention Se a, ee a, Positional Positional Encoding @ © @ Encoding Input Output Embedding Embedding Inputs Outputs (shifted right)\\n\\nFigure 1: The Transformer - model architecture.\\n\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.\\n\\n3.1 Encoder and Decoder Stacks\\n\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position- wise fully connected feed-forward network. We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512.\\n\\nDecoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.'],\n",
    "    \"\"\"The model incorporates the order of the sequence into its computations through the use of positional encoding. Since the Transformer architecture does not have any inherent recurrence or convolution to capture the sequence order, it explicitly adds positional encodings to the input embeddings.\n",
    "\n",
    "This positional encoding is designed to give the model information about the relative or absolute position of tokens in the sequence. It allows the attention mechanism to take into account the order of the sequence when performing operations. The addition of these encodings ensures that the model can differentiate between positions, which is crucial for tasks that require understanding the sequence dynamics, such as language modeling and translation.\n",
    "\n",
    "The relevant information can be found in Section 3.1, \"Encoder and Decoder Stacks,\" where it describes how the model architecture integrates positional encoding along with input embeddings (referenced in the figure).\"\"\",\n",
    "    \"\"\"Since our model contains no recurrence and no convolution, in order for the model to make use of the\n",
    "order of the sequence, we must inject some information about the relative or absolute position of the\n",
    "tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\n",
    "bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\n",
    "as the embeddings, so that the two can be summed. There are many choices of positional encodings,\n",
    "learned and fixed.\"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a853fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.loc[len(df2)] = [\n",
    "    \"What datasets did they use?\",\n",
    "    ['5 Training\\n\\nThis section describes the training regime for our models.\\n\\n5.1 Training Data and Batching\\n\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source- target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.\\n\\n5.2 Hardware and Schedule\\n\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days).\\n\\n5.3 Optimizer\\n\\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning rate over the course of training, according to the formula:\\n\\nlrate = d−0.5 model · min(step_num−0.5,step_num · warmup_steps−1.5) (3)\\n\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup_steps = 4000.\\n\\n5.4 Regularization\\n\\nWe employ three types of regularization during training:\\n\\n7\\n\\nTable 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\n\\nModel BLEU EN-DE EN-FR Training Cost (FLOPs) EN-DE EN-FR ByteNet [18] 23.75 Deep-Att + PosUnk [39] 39.2 1.0 · 1020 GNMT + RL [38] 24.6 39.92 2.3 · 1019 1.4 · 1020 ConvS2S [9] 25.16 40.46 9.6 · 1018 1.5 · 1020 MoE [32] 26.03 40.56 2.0 · 1019 1.2 · 1020 Deep-Att + PosUnk Ensemble [39] 40.4 8.0 · 1020 GNMT + RL Ensemble [38] 26.30 41.16 1.8 · 1020 1.1 · 1021 ConvS2S Ensemble [9] 26.36 41.29 7.7 · 1019 1.2 · 1021 Transformer (base model) 27.3 38.1 3.3 · 1018 Transformer (big) 28.4 41.8 2.3 · 1019\\n\\nResidual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of Pdrop = 0.1.\\n\\nLabel Smoothing During training, we employed label smoothing of value ϵls = 0.1 [36]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.'],\n",
    "    \"\"\"The training datasets used in the research are:\n",
    "\n",
    "WMT 2014 English-German dataset: This dataset consists of about 4.5 million sentence pairs. This information can be found in Section 5.1 Training Data and Batching.\n",
    "WMT 2014 English-French dataset: This is a significantly larger dataset consisting of 36 million sentences. Like the previous dataset, it was also mentioned in Section 5.1 Training Data and Batching.\n",
    "These datasets were used to train the models described in the document.\"\"\",\n",
    "    \"\"\"We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\n",
    "sentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared sourcetarget\n",
    "vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\n",
    "2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\n",
    "vocabulary [38]. Sentence pairs were batched together by approximate sequence length.\"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f0f871",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.loc[len(df2)] = [\n",
    "    \"How is attention computed efficiently in practice using matrices?\",\n",
    "    ['3.2 Attention\\n\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n\\n3\\n\\nScaled Dot-Product Attention\\n\\nMulti-Head Attention\\n\\nLinear\\n\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.\\n\\nof the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\\n\\n3.2.1 Scaled Dot-Product Attention\\n\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the √ dk, and apply a softmax function to obtain the weights on the query with all keys, divide each by values.\\n\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices K and V . We compute the matrix of outputs as:\\n\\nAttention(Q,K,V ) = softmax( QKT √ dk )V (1)\\n\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi- plicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor 1√ of . Additive attention computes the compatibility function using a feed-forward network with dk a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.\\n\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk [3]. We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients 4. To counteract this effect, we scale the dot products by 1√ . dk',\n",
    " '3.2.2 Multi-Head Attention\\n\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n\\n‘To illustrate why the dot products get large, assume that the components of q and k are independent random variables with mean 0 and variance 1. Then their dot product, g -k = ves, qiki, has mean 0 and variance dx.\\n\\n4\\n\\noutput values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2.\\n\\nMulti-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\\n\\nMultiHead(Q,K,V ) = Concat(head1,...,headh)W O where headi = Attention(QW Q i ,KW K i ,V W V i )\\n\\nWhere the projections are parameter matrices W Q and W O ∈ Rhdv×dmodel. i ∈ Rdmodel×dk, W K i ∈ Rdmodel×dk, W V i ∈ Rdmodel×dv\\n\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.\\n\\n3.2.3 Applications of Attention in our Model\\n\\nThe Transformer uses multi-head attention in three different ways:\\n\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9].\\n\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.\\n\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to −∞) all values in the input of the softmax which correspond to illegal connections. See Figure 2.'],\n",
    "    \"\"\"Attention is computed efficiently in practice using matrices through the Scaled Dot-Product Attention mechanism. Here’s how it works:\n",
    "\n",
    "Input Packing: The input consists of queries (Q), keys (K), and values (V). Instead of processing each query, key, and value individually, they are packed together into matrices. This allows for simultaneous computation.\n",
    "Dot Product Calculation: The attention function computes the dot products of the queries with the keys. Specifically, the formula used is:\n",
    "[\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{Q K^T}{\\sqrt{d_k}}\\right)V\n",
    "]\n",
    "In this equation, (QK^T) is the matrix multiplication that results in a matrix of dot products where each entry represents the compatibility between a query and a key.\n",
    "Scaling: The dot products are scaled by (\\sqrt{d_k}) to counteract the effect of large dot products. This scaling helps in maintaining gradients in a sensible range during training.\n",
    "Softmax Application: The scaled values are passed through a softmax function to obtain the weights for each query and its corresponding keys.\n",
    "Weighted Sum: Finally, these weights are applied to the values (V), resulting in the final output of the attention mechanism.\n",
    "This approach, utilizing optimized matrix multiplication routines, significantly enhances efficiency compared to computing attention for each query separately.\n",
    "\n",
    "This information can be found in Section 3.2.1 Scaled Dot-Product Attention on pages related to attention mechanisms in the document.\"\"\",\n",
    "    \"\"\"In practice, we compute the attention function on a set of queries simultaneously, packed together\n",
    "into a matrix Q. The keys and values are also packed together into matrices K and V . We compute\n",
    "the matrix of outputs as:\n",
    "Attention(Q,K, V ) = softmax(\n",
    "QKT\n",
    "√\n",
    "dk\n",
    ")V (1)\n",
    "The two most commonly used attention functions are additive attention [2], and dot-product (multiplicative)\n",
    "attention. Dot-product attention is identical to our algorithm, except for the scaling factor\n",
    "of √1\n",
    "dk\n",
    ". Additive attention computes the compatibility function using a feed-forward network with\n",
    "a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\n",
    "much faster and more space-efficient in practice, since it can be implemented using highly optimized\n",
    "matrix multiplication code.\n",
    "While for small values of dk the two mechanisms perform similarly, additive attention outperforms\n",
    "dot product attention without scaling for larger values of dk [3]. We suspect that for large values of\n",
    "dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\n",
    "extremely small gradients 4. To counteract this effect, we scale the dot products by √1\n",
    "dk\n",
    ".\"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ff6080",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.loc[len(df2)] = [\n",
    "    \"How did the Transformer (big) model perform on the WMT 2014 English-to-German task?\",\n",
    "    ['6 Results\\n\\n6.1 Machine Translation\\n\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is listed in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.\\n\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than 1/4 the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate Pdrop = 0.1, instead of 0.3.\\n\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We used beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters were chosen after experimentation on the development set. We set the maximum output length during inference to input length + 50, but terminate early when possible [38].\\n\\nTable 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature. We estimate the number of floating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each GPU 5.\\n\\n6.2 Model Variations\\n\\nTo evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the\\n\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n\\n8\\n\\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base model. All metrics are on the English-to-German translation development set, newstest2013. Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities.\\n\\nN dmodel dff h dk dv Pdrop ϵls train steps PPL (dev) BLEU params (dev) ×106 base 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65 1 512 512 5.29 24.9 (A) 4 16 128 32 128 32 5.00 4.91 25.5 25.8 32 16 16 5.01 25.4 (B) 16 32 5.16 5.01 25.1 25.4 58 60 2 6.11 23.7 36 4 5.19 25.3 50 8 4.88 25.5 80 (C) 256 32 32 5.75 24.5 28 1024 128 128 4.66 26.0 168 1024 5.12 25.4 53 4096 4.75 26.2 90 0.0 5.77 24.6 (D) 0.2 0.0 4.95 4.67 25.5 25.3 0.2 5.47 25.7 (E) positional embedding instead of sinusoids 4.92 25.7',\n",
    " '7 Conclusion\\n\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.\\n\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles.\\n\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours.\\n\\nThe code we used to train and evaluate our models is available at https://github.com/ tensorflow/tensor2tensor.\\n\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration.\\n\\nReferences\\n\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.\\n\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. CoRR, abs/1409.0473, 2014.\\n\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural machine translation architectures. CoRR, abs/1703.03906, 2017.\\n\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine reading. arXiv preprint arXiv:1601.06733, 2016.\\n\\n10\\n\\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. CoRR, abs/1406.1078, 2014.\\n\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv preprint arXiv:1610.02357, 2016.\\n\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n\\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural network grammars. In Proc. of NAACL, 2016.\\n\\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu- tional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013.\\n\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im- age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770–778, 2016.\\n\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies, 2001.\\n\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780, 1997.\\n\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 832–841. ACL, August 2009.\\n\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural Information Processing Systems, (NIPS), 2016.\\n\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference on Learning Representations (ICLR), 2016.\\n\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko- ray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2, 2017.\\n\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks. In International Conference on Learning Representations, 2017.\\n\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint arXiv:1703.10722, 2017.\\n\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint arXiv:1703.03130, 2017.\\n\\n[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task sequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention- based neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n\\n11\\n\\n[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated corpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 152–159. ACL, June 2006.\\n\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model. In Empirical Methods in Natural Language Processing, 2016.\\n\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive summarization. arXiv preprint arXiv:1705.04304, 2017.\\n\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July 2006.\\n\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv preprint arXiv:1608.05859, 2016.\\n\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909, 2015.\\n\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017.\\n\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi- nov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(1):1929–1958, 2014.\\n\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates, Inc., 2015.\\n\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.',\n",
    " '5 Training\\n\\nThis section describes the training regime for our models.\\n\\n5.1 Training Data and Batching\\n\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source- target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.\\n\\n5.2 Hardware and Schedule\\n\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days).\\n\\n5.3 Optimizer\\n\\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning rate over the course of training, according to the formula:\\n\\nlrate = d−0.5 model · min(step_num−0.5,step_num · warmup_steps−1.5) (3)\\n\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup_steps = 4000.\\n\\n5.4 Regularization\\n\\nWe employ three types of regularization during training:\\n\\n7\\n\\nTable 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\n\\nModel BLEU EN-DE EN-FR Training Cost (FLOPs) EN-DE EN-FR ByteNet [18] 23.75 Deep-Att + PosUnk [39] 39.2 1.0 · 1020 GNMT + RL [38] 24.6 39.92 2.3 · 1019 1.4 · 1020 ConvS2S [9] 25.16 40.46 9.6 · 1018 1.5 · 1020 MoE [32] 26.03 40.56 2.0 · 1019 1.2 · 1020 Deep-Att + PosUnk Ensemble [39] 40.4 8.0 · 1020 GNMT + RL Ensemble [38] 26.30 41.16 1.8 · 1020 1.1 · 1021 ConvS2S Ensemble [9] 26.36 41.29 7.7 · 1019 1.2 · 1021 Transformer (base model) 27.3 38.1 3.3 · 1018 Transformer (big) 28.4 41.8 2.3 · 1019\\n\\nResidual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of Pdrop = 0.1.\\n\\nLabel Smoothing During training, we employed label smoothing of value ϵls = 0.1 [36]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.'],\n",
    "    \"\"\"The Transformer (big) model achieved a BLEU score of 28.4 on the WMT 2014 English-to-German translation task, outperforming the best previously reported models (including ensembles) by more than 2.0 BLEU points, thus establishing a new state-of-the-art BLEU score. This information can be found in Section 6.1 Machine Translation on page 6.\"\"\",\n",
    "    \"\"\"The Transformer (big) model achieved a BLEU score of 28.4, outperforming previous models by over 2.0 BLEU points.\n",
    "This includes surpassing ensembles and establishing a new state of the art.\n",
    "It was trained in 3.5 days using 8 P100 GPUs.\"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edd2cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.loc[len(df2)] = [\n",
    "    \"What BLEU score did the Transformer (big) model achieve on the English-to-French task?\",\n",
    "    ['6 Results\\n\\n6.1 Machine Translation\\n\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is listed in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.\\n\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than 1/4 the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate Pdrop = 0.1, instead of 0.3.\\n\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We used beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters were chosen after experimentation on the development set. We set the maximum output length during inference to input length + 50, but terminate early when possible [38].\\n\\nTable 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature. We estimate the number of floating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each GPU 5.\\n\\n6.2 Model Variations\\n\\nTo evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the\\n\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n\\n8\\n\\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base model. All metrics are on the English-to-German translation development set, newstest2013. Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities.\\n\\nN dmodel dff h dk dv Pdrop ϵls train steps PPL (dev) BLEU params (dev) ×106 base 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65 1 512 512 5.29 24.9 (A) 4 16 128 32 128 32 5.00 4.91 25.5 25.8 32 16 16 5.01 25.4 (B) 16 32 5.16 5.01 25.1 25.4 58 60 2 6.11 23.7 36 4 5.19 25.3 50 8 4.88 25.5 80 (C) 256 32 32 5.75 24.5 28 1024 128 128 4.66 26.0 168 1024 5.12 25.4 53 4096 4.75 26.2 90 0.0 5.77 24.6 (D) 0.2 0.0 4.95 4.67 25.5 25.3 0.2 5.47 25.7 (E) positional embedding instead of sinusoids 4.92 25.7',\n",
    " '3\\n\\n2023\\n\\n2\\n\\n0\\n\\n2\\n\\ng u A 2 ] L C . s c [ 7 v 2 6 7 3 0 . 6 0\\n\\n7\\n\\n1\\n\\n:\\n\\nv\\n\\narXiv\\n\\ni\\n\\nX\\n\\nr\\n\\na\\n\\nProvided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works.\\n\\nAttention Is All You Need\\n\\nAshish Vaswani∗\\n\\nGoogle Brain\\n\\navaswani@google.com\\n\\nNoam Shazeer∗ Google Brain noam@google.com\\n\\nNiki Parmar∗ Google Research nikip@google.com\\n\\nJakob Uszkoreit∗\\n\\nGoogle Research usz@google.com\\n\\nLlion Jones∗\\n\\nGoogle Research llion@google.com\\n\\nAidan N. Gomez∗ † University of Toronto aidan@cs.toronto.edu\\n\\nŁukasz Kaiser∗ Google Brain lukaszkaiser@google.com\\n\\nIllia Polosukhin∗ ‡\\n\\nillia.polosukhin@gmail.com\\n\\nAbstract\\n\\nThe dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English- to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\\n\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.\\n\\n†Work performed while at Google Brain.\\n\\n‡Work performed while at Google Research.\\n\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.'],\n",
    "    \"\"\"The Transformer (big) model achieved a BLEU score of 41.0 on the WMT 2014 English-to-French translation task. This information can be found in Section 6.1 Machine Translation on page 6.\"\"\",\n",
    "    \"\"\"It achieved a BLEU score of 41.0 on the WMT 2014 English-to-French task.\n",
    "This outperforms all previously published single models.\n",
    "It required less than 25% of the training cost of the former top-performing model.\"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd7e267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregunta</th>\n",
       "      <th>Contexto</th>\n",
       "      <th>Respuesta_Modelo</th>\n",
       "      <th>Respuesta_Referencia</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>answer_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>¿Qué es una RNN?</td>\n",
       "      <td>[2.3 Ejemplo RNN\\n\\nEstas redes se suelen util...</td>\n",
       "      <td>Una RNN, o red neuronal recurrente, es un tipo...</td>\n",
       "      <td>Las Redes Neuronales Recurrentes son un tipo e...</td>\n",
       "      <td>0.814973</td>\n",
       "      <td>0.777145</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.815128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>¿Qué limitaciones tienen las RNNs?</td>\n",
       "      <td>[1.1 Problemas de las RNN est´andar\\n\\nEstas r...</td>\n",
       "      <td>Las RNNs (Redes Neuronales Recurrentes) enfren...</td>\n",
       "      <td>Estas redes se enfrentan a un problema fundame...</td>\n",
       "      <td>0.768091</td>\n",
       "      <td>0.764773</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.903567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>¿Qué son las LSTM?</td>\n",
       "      <td>[3.3 LSTM\\n\\nImagen de flujo de informaci´on e...</td>\n",
       "      <td>Las LSTM (Long Short-Term Memory) son un tipo ...</td>\n",
       "      <td>Las LSTMs (Long Short-Term Memory) son un tipo...</td>\n",
       "      <td>0.797648</td>\n",
       "      <td>0.815644</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.938347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>¿Cuáles son las componentes clave de las LSTM?</td>\n",
       "      <td>[3.2 Componentes clave de las LSTM\\n\\nLos comp...</td>\n",
       "      <td>Las componentes clave de las LSTM son:\\n\\nCélu...</td>\n",
       "      <td>Los componentes clave de las LSTM que las dist...</td>\n",
       "      <td>0.702831</td>\n",
       "      <td>0.794647</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.931384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>¿Qué son las redes GRU?</td>\n",
       "      <td>[4.3 GRU\\n\\nLas Gated Recurrent Units (GRU) so...</td>\n",
       "      <td>Las Gated Recurrent Units (GRU) son un tipo de...</td>\n",
       "      <td>Las Gated Recurrent Units (GRU) son un tipo de...</td>\n",
       "      <td>0.794439</td>\n",
       "      <td>0.683109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.974084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Proporcióname una comparación entre las LSTM y...</td>\n",
       "      <td>[4.3 GRU\\n\\nLas Gated Recurrent Units (GRU) so...</td>\n",
       "      <td>Las LSTM (Long Short-Term Memory) y las GRU (G...</td>\n",
       "      <td>Comparación entre LSTM y GRU\\n• Complejidad: E...</td>\n",
       "      <td>0.866765</td>\n",
       "      <td>0.790532</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.956360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>¿Cuáles son las limitaciones de las LSTM?</td>\n",
       "      <td>[6 Limitaciones y mejores pr´acticas de LSTM\\n...</td>\n",
       "      <td>Las limitaciones de las LSTM son:\\n\\nComplejid...</td>\n",
       "      <td>Limitaciones de las LSTM\\n• Complejidad y cost...</td>\n",
       "      <td>0.749903</td>\n",
       "      <td>0.819090</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.958100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Respecto al ajuste de hiperparámetros, ¿qué me...</td>\n",
       "      <td>[6 Limitaciones y mejores pr´acticas de LSTM\\n...</td>\n",
       "      <td>Respecto al ajuste de hiperparámetros en LSTM,...</td>\n",
       "      <td>Ajuste de hiperparámetros:\\n- Tamaño del númer...</td>\n",
       "      <td>0.669974</td>\n",
       "      <td>0.839722</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.890632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>¿Hay alguna técnica para reducir sobreajuste e...</td>\n",
       "      <td>[6 Limitaciones y mejores pr´acticas de LSTM\\n...</td>\n",
       "      <td>Hay varias técnicas para reducir el sobreajust...</td>\n",
       "      <td>Uso de regulación para evitar sobreajuste:\\n- ...</td>\n",
       "      <td>0.606070</td>\n",
       "      <td>0.839068</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.893565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Dame 4 casos de uso de las LSTM</td>\n",
       "      <td>[6 Limitaciones y mejores pr´acticas de LSTM\\n...</td>\n",
       "      <td>Los casos de uso de las LSTM incluyen:\\n\\nTrad...</td>\n",
       "      <td>Casos de uso de las LSTM\\n• Dependencias tempo...</td>\n",
       "      <td>0.682564</td>\n",
       "      <td>0.716270</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.872708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>What is a attention function?</td>\n",
       "      <td>[3.2 Attention\\n\\nAn attention function can be...</td>\n",
       "      <td>An attention function is a mechanism that maps...</td>\n",
       "      <td>An attention function can be described as mapp...</td>\n",
       "      <td>0.583461</td>\n",
       "      <td>0.727181</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.934127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Tell me applications of Attention in our Model</td>\n",
       "      <td>[3.2.2 Multi-Head Attention\\n\\nInstead of perf...</td>\n",
       "      <td>The Transformer model uses attention in three ...</td>\n",
       "      <td>The Transformer uses multi-head attention in t...</td>\n",
       "      <td>0.624016</td>\n",
       "      <td>0.516775</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.898642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>What is Multi-Head Attention?</td>\n",
       "      <td>[3.2.2 Multi-Head Attention\\n\\nInstead of perf...</td>\n",
       "      <td>Multi-Head Attention is an attention mechanism...</td>\n",
       "      <td>Instead of performing a single attention funct...</td>\n",
       "      <td>0.653579</td>\n",
       "      <td>0.765651</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.878023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Why self-attention?</td>\n",
       "      <td>[4 Why Self-Attention\\n\\nIn this section we co...</td>\n",
       "      <td>Self-attention is favored for several reasons ...</td>\n",
       "      <td>In this section we compare various aspects of ...</td>\n",
       "      <td>0.618472</td>\n",
       "      <td>0.609084</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.842773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Each of the layers in our encoder and decoder ...</td>\n",
       "      <td>[3.3 Position-wise Feed-Forward Networks\\n\\nIn...</td>\n",
       "      <td>Each layer in the encoder and decoder of the T...</td>\n",
       "      <td>In addition to attention sub-layers, each of t...</td>\n",
       "      <td>0.720588</td>\n",
       "      <td>0.698126</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.850841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>In what way does the model incorporate the ord...</td>\n",
       "      <td>[1 Introduction\\n\\nRecurrent neural networks, ...</td>\n",
       "      <td>The model incorporates the order of the sequen...</td>\n",
       "      <td>Since our model contains no recurrence and no ...</td>\n",
       "      <td>0.674769</td>\n",
       "      <td>0.638088</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.784113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>What datasets did they use?</td>\n",
       "      <td>[5 Training\\n\\nThis section describes the trai...</td>\n",
       "      <td>The training datasets used in the research are...</td>\n",
       "      <td>We trained on the standard WMT 2014 English-Ge...</td>\n",
       "      <td>0.605352</td>\n",
       "      <td>0.512609</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.789078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>How is attention computed efficiently in pract...</td>\n",
       "      <td>[3.2 Attention\\n\\nAn attention function can be...</td>\n",
       "      <td>Attention is computed efficiently in practice ...</td>\n",
       "      <td>In practice, we compute the attention function...</td>\n",
       "      <td>0.711568</td>\n",
       "      <td>0.670940</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.844505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>How did the Transformer (big) model perform on...</td>\n",
       "      <td>[6 Results\\n\\n6.1 Machine Translation\\n\\nOn th...</td>\n",
       "      <td>The Transformer (big) model achieved a BLEU sc...</td>\n",
       "      <td>The Transformer (big) model achieved a BLEU sc...</td>\n",
       "      <td>0.655258</td>\n",
       "      <td>0.792248</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.821527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>What BLEU score did the Transformer (big) mode...</td>\n",
       "      <td>[6 Results\\n\\n6.1 Machine Translation\\n\\nOn th...</td>\n",
       "      <td>The Transformer (big) model achieved a BLEU sc...</td>\n",
       "      <td>It achieved a BLEU score of 41.0 on the WMT 20...</td>\n",
       "      <td>0.605911</td>\n",
       "      <td>0.853187</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.723984</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Pregunta  \\\n",
       "0                                    ¿Qué es una RNN?   \n",
       "1                  ¿Qué limitaciones tienen las RNNs?   \n",
       "2                                  ¿Qué son las LSTM?   \n",
       "3      ¿Cuáles son las componentes clave de las LSTM?   \n",
       "4                             ¿Qué son las redes GRU?   \n",
       "5   Proporcióname una comparación entre las LSTM y...   \n",
       "6           ¿Cuáles son las limitaciones de las LSTM?   \n",
       "7   Respecto al ajuste de hiperparámetros, ¿qué me...   \n",
       "8   ¿Hay alguna técnica para reducir sobreajuste e...   \n",
       "9                     Dame 4 casos de uso de las LSTM   \n",
       "10                      What is a attention function?   \n",
       "11     Tell me applications of Attention in our Model   \n",
       "12                      What is Multi-Head Attention?   \n",
       "13                                Why self-attention?   \n",
       "14  Each of the layers in our encoder and decoder ...   \n",
       "15  In what way does the model incorporate the ord...   \n",
       "16                        What datasets did they use?   \n",
       "17  How is attention computed efficiently in pract...   \n",
       "18  How did the Transformer (big) model perform on...   \n",
       "19  What BLEU score did the Transformer (big) mode...   \n",
       "\n",
       "                                             Contexto  \\\n",
       "0   [2.3 Ejemplo RNN\\n\\nEstas redes se suelen util...   \n",
       "1   [1.1 Problemas de las RNN est´andar\\n\\nEstas r...   \n",
       "2   [3.3 LSTM\\n\\nImagen de flujo de informaci´on e...   \n",
       "3   [3.2 Componentes clave de las LSTM\\n\\nLos comp...   \n",
       "4   [4.3 GRU\\n\\nLas Gated Recurrent Units (GRU) so...   \n",
       "5   [4.3 GRU\\n\\nLas Gated Recurrent Units (GRU) so...   \n",
       "6   [6 Limitaciones y mejores pr´acticas de LSTM\\n...   \n",
       "7   [6 Limitaciones y mejores pr´acticas de LSTM\\n...   \n",
       "8   [6 Limitaciones y mejores pr´acticas de LSTM\\n...   \n",
       "9   [6 Limitaciones y mejores pr´acticas de LSTM\\n...   \n",
       "10  [3.2 Attention\\n\\nAn attention function can be...   \n",
       "11  [3.2.2 Multi-Head Attention\\n\\nInstead of perf...   \n",
       "12  [3.2.2 Multi-Head Attention\\n\\nInstead of perf...   \n",
       "13  [4 Why Self-Attention\\n\\nIn this section we co...   \n",
       "14  [3.3 Position-wise Feed-Forward Networks\\n\\nIn...   \n",
       "15  [1 Introduction\\n\\nRecurrent neural networks, ...   \n",
       "16  [5 Training\\n\\nThis section describes the trai...   \n",
       "17  [3.2 Attention\\n\\nAn attention function can be...   \n",
       "18  [6 Results\\n\\n6.1 Machine Translation\\n\\nOn th...   \n",
       "19  [6 Results\\n\\n6.1 Machine Translation\\n\\nOn th...   \n",
       "\n",
       "                                     Respuesta_Modelo  \\\n",
       "0   Una RNN, o red neuronal recurrente, es un tipo...   \n",
       "1   Las RNNs (Redes Neuronales Recurrentes) enfren...   \n",
       "2   Las LSTM (Long Short-Term Memory) son un tipo ...   \n",
       "3   Las componentes clave de las LSTM son:\\n\\nCélu...   \n",
       "4   Las Gated Recurrent Units (GRU) son un tipo de...   \n",
       "5   Las LSTM (Long Short-Term Memory) y las GRU (G...   \n",
       "6   Las limitaciones de las LSTM son:\\n\\nComplejid...   \n",
       "7   Respecto al ajuste de hiperparámetros en LSTM,...   \n",
       "8   Hay varias técnicas para reducir el sobreajust...   \n",
       "9   Los casos de uso de las LSTM incluyen:\\n\\nTrad...   \n",
       "10  An attention function is a mechanism that maps...   \n",
       "11  The Transformer model uses attention in three ...   \n",
       "12  Multi-Head Attention is an attention mechanism...   \n",
       "13  Self-attention is favored for several reasons ...   \n",
       "14  Each layer in the encoder and decoder of the T...   \n",
       "15  The model incorporates the order of the sequen...   \n",
       "16  The training datasets used in the research are...   \n",
       "17  Attention is computed efficiently in practice ...   \n",
       "18  The Transformer (big) model achieved a BLEU sc...   \n",
       "19  The Transformer (big) model achieved a BLEU sc...   \n",
       "\n",
       "                                 Respuesta_Referencia  faithfulness  \\\n",
       "0   Las Redes Neuronales Recurrentes son un tipo e...      0.814973   \n",
       "1   Estas redes se enfrentan a un problema fundame...      0.768091   \n",
       "2   Las LSTMs (Long Short-Term Memory) son un tipo...      0.797648   \n",
       "3   Los componentes clave de las LSTM que las dist...      0.702831   \n",
       "4   Las Gated Recurrent Units (GRU) son un tipo de...      0.794439   \n",
       "5   Comparación entre LSTM y GRU\\n• Complejidad: E...      0.866765   \n",
       "6   Limitaciones de las LSTM\\n• Complejidad y cost...      0.749903   \n",
       "7   Ajuste de hiperparámetros:\\n- Tamaño del númer...      0.669974   \n",
       "8   Uso de regulación para evitar sobreajuste:\\n- ...      0.606070   \n",
       "9   Casos de uso de las LSTM\\n• Dependencias tempo...      0.682564   \n",
       "10  An attention function can be described as mapp...      0.583461   \n",
       "11  The Transformer uses multi-head attention in t...      0.624016   \n",
       "12  Instead of performing a single attention funct...      0.653579   \n",
       "13  In this section we compare various aspects of ...      0.618472   \n",
       "14  In addition to attention sub-layers, each of t...      0.720588   \n",
       "15  Since our model contains no recurrence and no ...      0.674769   \n",
       "16  We trained on the standard WMT 2014 English-Ge...      0.605352   \n",
       "17  In practice, we compute the attention function...      0.711568   \n",
       "18  The Transformer (big) model achieved a BLEU sc...      0.655258   \n",
       "19  It achieved a BLEU score of 41.0 on the WMT 20...      0.605911   \n",
       "\n",
       "    answer_relevancy  context_precision  answer_similarity  \n",
       "0           0.777145           1.000000           0.815128  \n",
       "1           0.764773           1.000000           0.903567  \n",
       "2           0.815644           1.000000           0.938347  \n",
       "3           0.794647           1.000000           0.931384  \n",
       "4           0.683109           0.000000           0.974084  \n",
       "5           0.790532           1.000000           0.956360  \n",
       "6           0.819090           1.000000           0.958100  \n",
       "7           0.839722           0.666667           0.890632  \n",
       "8           0.839068           0.333333           0.893565  \n",
       "9           0.716270           0.333333           0.872708  \n",
       "10          0.727181           0.000000           0.934127  \n",
       "11          0.516775           0.000000           0.898642  \n",
       "12          0.765651           0.500000           0.878023  \n",
       "13          0.609084           0.000000           0.842773  \n",
       "14          0.698126           0.500000           0.850841  \n",
       "15          0.638088           0.000000           0.784113  \n",
       "16          0.512609           0.000000           0.789078  \n",
       "17          0.670940           0.000000           0.844505  \n",
       "18          0.792248           0.666667           0.821527  \n",
       "19          0.853187           0.500000           0.723984  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "api_version = os.environ[\"OPENAI_API_VERSION\"]\n",
    "endpoint = os.environ[\"OPENAI_API_BASE\"]\n",
    "subscription_key = os.environ[\"API_KEY\"]\n",
    "deployment = os.environ[\"OPENAI_DEPLOYMENT_NAME\"]\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_version=api_version,\n",
    "    azure_endpoint=endpoint,\n",
    "    api_key=subscription_key,\n",
    ")\n",
    "\n",
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=model,\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return dot(a, b) / (norm(a) * norm(b))\n",
    "\n",
    "def evaluate_row(row):\n",
    "    query = row[\"Pregunta\"]\n",
    "    generated_answer = row[\"Respuesta_Modelo\"]\n",
    "    expected_answer = row[\"Respuesta_Referencia\"]\n",
    "    context = row[\"Contexto\"]\n",
    "    \n",
    "    # Si context es lista, se concatena\n",
    "    if isinstance(context, list):\n",
    "        context_concat = \" \".join(context)\n",
    "        context_docs = context\n",
    "    else:\n",
    "        context_concat = context\n",
    "        context_docs = [context_concat]\n",
    "    \n",
    "    # Obtenemos los embeddings y los convertimos a numpy arrays\n",
    "    emb_query = np.array(get_embedding(query))\n",
    "    emb_generated = np.array(get_embedding(generated_answer))\n",
    "    emb_expected = np.array(get_embedding(expected_answer))\n",
    "    emb_context_concat = np.array(get_embedding(context_concat))\n",
    "    emb_context_docs = [np.array(get_embedding(doc)) for doc in context_docs]\n",
    "    \n",
    "    # answer_relevancy: query vs respuesta generada\n",
    "    answer_relevancy = cosine_similarity(emb_query, emb_generated)\n",
    "    \n",
    "    # answer_similarity: respuesta generada vs respuesta esperada\n",
    "    answer_similarity = cosine_similarity(emb_generated, emb_expected)\n",
    "    \n",
    "    # faithfulness: respuesta generada vs contexto (concatenado)\n",
    "    faithfulness = cosine_similarity(emb_generated, emb_context_concat)\n",
    "    \n",
    "    # context_precision: % documentos relevantes recuperados (similitud query-doc > umbral)\n",
    "    threshold = 0.60\n",
    "    relevant_docs = sum(cosine_similarity(emb_query, doc_emb) > threshold for doc_emb in emb_context_docs)\n",
    "    context_precision = relevant_docs / len(emb_context_docs) if emb_context_docs else 0\n",
    "    \n",
    "    return pd.Series({\n",
    "        \"faithfulness\": faithfulness,\n",
    "        \"answer_relevancy\": answer_relevancy,\n",
    "        \"context_precision\": context_precision,\n",
    "        \"answer_similarity\": answer_similarity,\n",
    "    })\n",
    "\n",
    "# Aplicamos la evaluación fila a fila \n",
    "df_metrics = df.apply(evaluate_row, axis=1)\n",
    "df_result = pd.concat([df, df_metrics], axis=1)\n",
    "\n",
    "\n",
    "df_metrics2 = df2.apply(evaluate_row, axis=1)\n",
    "df_result2 = pd.concat([df2, df_metrics2], axis=1)\n",
    "\n",
    "\n",
    "df_concatenado = pd.concat([df_result, df_result2], ignore_index=True)\n",
    "df_concatenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275559a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from bert_score import score\n",
    "\n",
    "# Calculamos BERTScore\n",
    "_, _, F1_1 = score(df['Respuesta_Modelo'].tolist(), df['Respuesta_Referencia'].tolist(), lang=\"es\")\n",
    "\n",
    "# Añadios la evaluacion BERTScore a un DataFrame auxiliar\n",
    "df1_new = pd.DataFrame({'BERTScore_F1': [f.item() for f in F1_1]})\n",
    "\n",
    "\n",
    "# Lo mismo con df2\n",
    "_, _, F1_2 = score(df2['Respuesta_Modelo'].tolist(), df2['Respuesta_Referencia'].tolist(), lang=\"en\")\n",
    "df2_new = pd.DataFrame({'BERTScore_F1': [f.item() for f in F1_2]})\n",
    "\n",
    "# Los concateno\n",
    "bert_concat = pd.concat([df1_new, df2_new], ignore_index=True)\n",
    "\n",
    "# Lo añado al total\n",
    "df_concatenado[\"BERTScore_F1\"] = bert_concat[\"BERTScore_F1\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "28cb0d6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregunta</th>\n",
       "      <th>Contexto</th>\n",
       "      <th>Respuesta_Modelo</th>\n",
       "      <th>Respuesta_Referencia</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>answer_similarity</th>\n",
       "      <th>BERTScore_F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>¿Qué es una RNN?</td>\n",
       "      <td>[2.3 Ejemplo RNN\\n\\nEstas redes se suelen util...</td>\n",
       "      <td>Una RNN, o red neuronal recurrente, es un tipo...</td>\n",
       "      <td>Las Redes Neuronales Recurrentes son un tipo e...</td>\n",
       "      <td>0.814973</td>\n",
       "      <td>0.777145</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.815128</td>\n",
       "      <td>0.710676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>¿Qué limitaciones tienen las RNNs?</td>\n",
       "      <td>[1.1 Problemas de las RNN est´andar\\n\\nEstas r...</td>\n",
       "      <td>Las RNNs (Redes Neuronales Recurrentes) enfren...</td>\n",
       "      <td>Estas redes se enfrentan a un problema fundame...</td>\n",
       "      <td>0.768091</td>\n",
       "      <td>0.764773</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.903567</td>\n",
       "      <td>0.777857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>¿Qué son las LSTM?</td>\n",
       "      <td>[3.3 LSTM\\n\\nImagen de flujo de informaci´on e...</td>\n",
       "      <td>Las LSTM (Long Short-Term Memory) son un tipo ...</td>\n",
       "      <td>Las LSTMs (Long Short-Term Memory) son un tipo...</td>\n",
       "      <td>0.797648</td>\n",
       "      <td>0.815644</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.938347</td>\n",
       "      <td>0.831891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>¿Cuáles son las componentes clave de las LSTM?</td>\n",
       "      <td>[3.2 Componentes clave de las LSTM\\n\\nLos comp...</td>\n",
       "      <td>Las componentes clave de las LSTM son:\\n\\nCélu...</td>\n",
       "      <td>Los componentes clave de las LSTM que las dist...</td>\n",
       "      <td>0.702831</td>\n",
       "      <td>0.794647</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.931384</td>\n",
       "      <td>0.758998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>¿Qué son las redes GRU?</td>\n",
       "      <td>[4.3 GRU\\n\\nLas Gated Recurrent Units (GRU) so...</td>\n",
       "      <td>Las Gated Recurrent Units (GRU) son un tipo de...</td>\n",
       "      <td>Las Gated Recurrent Units (GRU) son un tipo de...</td>\n",
       "      <td>0.794439</td>\n",
       "      <td>0.683109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.974084</td>\n",
       "      <td>0.901622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Proporcióname una comparación entre las LSTM y...</td>\n",
       "      <td>[4.3 GRU\\n\\nLas Gated Recurrent Units (GRU) so...</td>\n",
       "      <td>Las LSTM (Long Short-Term Memory) y las GRU (G...</td>\n",
       "      <td>Comparación entre LSTM y GRU\\n• Complejidad: E...</td>\n",
       "      <td>0.866765</td>\n",
       "      <td>0.790532</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.956360</td>\n",
       "      <td>0.797000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>¿Cuáles son las limitaciones de las LSTM?</td>\n",
       "      <td>[6 Limitaciones y mejores pr´acticas de LSTM\\n...</td>\n",
       "      <td>Las limitaciones de las LSTM son:\\n\\nComplejid...</td>\n",
       "      <td>Limitaciones de las LSTM\\n• Complejidad y cost...</td>\n",
       "      <td>0.749903</td>\n",
       "      <td>0.819090</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.958100</td>\n",
       "      <td>0.873939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Respecto al ajuste de hiperparámetros, ¿qué me...</td>\n",
       "      <td>[6 Limitaciones y mejores pr´acticas de LSTM\\n...</td>\n",
       "      <td>Respecto al ajuste de hiperparámetros en LSTM,...</td>\n",
       "      <td>Ajuste de hiperparámetros:\\n- Tamaño del númer...</td>\n",
       "      <td>0.669974</td>\n",
       "      <td>0.839722</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.890632</td>\n",
       "      <td>0.817970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>¿Hay alguna técnica para reducir sobreajuste e...</td>\n",
       "      <td>[6 Limitaciones y mejores pr´acticas de LSTM\\n...</td>\n",
       "      <td>Hay varias técnicas para reducir el sobreajust...</td>\n",
       "      <td>Uso de regulación para evitar sobreajuste:\\n- ...</td>\n",
       "      <td>0.606070</td>\n",
       "      <td>0.839068</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.893565</td>\n",
       "      <td>0.820560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Dame 4 casos de uso de las LSTM</td>\n",
       "      <td>[6 Limitaciones y mejores pr´acticas de LSTM\\n...</td>\n",
       "      <td>Los casos de uso de las LSTM incluyen:\\n\\nTrad...</td>\n",
       "      <td>Casos de uso de las LSTM\\n• Dependencias tempo...</td>\n",
       "      <td>0.682564</td>\n",
       "      <td>0.716270</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.872708</td>\n",
       "      <td>0.733046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>What is a attention function?</td>\n",
       "      <td>[3.2 Attention\\n\\nAn attention function can be...</td>\n",
       "      <td>An attention function is a mechanism that maps...</td>\n",
       "      <td>An attention function can be described as mapp...</td>\n",
       "      <td>0.583461</td>\n",
       "      <td>0.727181</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.934127</td>\n",
       "      <td>0.923933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Tell me applications of Attention in our Model</td>\n",
       "      <td>[3.2.2 Multi-Head Attention\\n\\nInstead of perf...</td>\n",
       "      <td>The Transformer model uses attention in three ...</td>\n",
       "      <td>The Transformer uses multi-head attention in t...</td>\n",
       "      <td>0.624016</td>\n",
       "      <td>0.516775</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.898642</td>\n",
       "      <td>0.894265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>What is Multi-Head Attention?</td>\n",
       "      <td>[3.2.2 Multi-Head Attention\\n\\nInstead of perf...</td>\n",
       "      <td>Multi-Head Attention is an attention mechanism...</td>\n",
       "      <td>Instead of performing a single attention funct...</td>\n",
       "      <td>0.653579</td>\n",
       "      <td>0.765651</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.878023</td>\n",
       "      <td>0.911197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Why self-attention?</td>\n",
       "      <td>[4 Why Self-Attention\\n\\nIn this section we co...</td>\n",
       "      <td>Self-attention is favored for several reasons ...</td>\n",
       "      <td>In this section we compare various aspects of ...</td>\n",
       "      <td>0.618472</td>\n",
       "      <td>0.609084</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.842773</td>\n",
       "      <td>0.859169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Each of the layers in our encoder and decoder ...</td>\n",
       "      <td>[3.3 Position-wise Feed-Forward Networks\\n\\nIn...</td>\n",
       "      <td>Each layer in the encoder and decoder of the T...</td>\n",
       "      <td>In addition to attention sub-layers, each of t...</td>\n",
       "      <td>0.720588</td>\n",
       "      <td>0.698126</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.850841</td>\n",
       "      <td>0.900421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>In what way does the model incorporate the ord...</td>\n",
       "      <td>[1 Introduction\\n\\nRecurrent neural networks, ...</td>\n",
       "      <td>The model incorporates the order of the sequen...</td>\n",
       "      <td>Since our model contains no recurrence and no ...</td>\n",
       "      <td>0.674769</td>\n",
       "      <td>0.638088</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.784113</td>\n",
       "      <td>0.870603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>What datasets did they use?</td>\n",
       "      <td>[5 Training\\n\\nThis section describes the trai...</td>\n",
       "      <td>The training datasets used in the research are...</td>\n",
       "      <td>We trained on the standard WMT 2014 English-Ge...</td>\n",
       "      <td>0.605352</td>\n",
       "      <td>0.512609</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.789078</td>\n",
       "      <td>0.859309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>How is attention computed efficiently in pract...</td>\n",
       "      <td>[3.2 Attention\\n\\nAn attention function can be...</td>\n",
       "      <td>Attention is computed efficiently in practice ...</td>\n",
       "      <td>In practice, we compute the attention function...</td>\n",
       "      <td>0.711568</td>\n",
       "      <td>0.670940</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.844505</td>\n",
       "      <td>0.840961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>How did the Transformer (big) model perform on...</td>\n",
       "      <td>[6 Results\\n\\n6.1 Machine Translation\\n\\nOn th...</td>\n",
       "      <td>The Transformer (big) model achieved a BLEU sc...</td>\n",
       "      <td>The Transformer (big) model achieved a BLEU sc...</td>\n",
       "      <td>0.655258</td>\n",
       "      <td>0.792248</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.821527</td>\n",
       "      <td>0.918985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>What BLEU score did the Transformer (big) mode...</td>\n",
       "      <td>[6 Results\\n\\n6.1 Machine Translation\\n\\nOn th...</td>\n",
       "      <td>The Transformer (big) model achieved a BLEU sc...</td>\n",
       "      <td>It achieved a BLEU score of 41.0 on the WMT 20...</td>\n",
       "      <td>0.605911</td>\n",
       "      <td>0.853187</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.723984</td>\n",
       "      <td>0.898398</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Pregunta  \\\n",
       "0                                    ¿Qué es una RNN?   \n",
       "1                  ¿Qué limitaciones tienen las RNNs?   \n",
       "2                                  ¿Qué son las LSTM?   \n",
       "3      ¿Cuáles son las componentes clave de las LSTM?   \n",
       "4                             ¿Qué son las redes GRU?   \n",
       "5   Proporcióname una comparación entre las LSTM y...   \n",
       "6           ¿Cuáles son las limitaciones de las LSTM?   \n",
       "7   Respecto al ajuste de hiperparámetros, ¿qué me...   \n",
       "8   ¿Hay alguna técnica para reducir sobreajuste e...   \n",
       "9                     Dame 4 casos de uso de las LSTM   \n",
       "10                      What is a attention function?   \n",
       "11     Tell me applications of Attention in our Model   \n",
       "12                      What is Multi-Head Attention?   \n",
       "13                                Why self-attention?   \n",
       "14  Each of the layers in our encoder and decoder ...   \n",
       "15  In what way does the model incorporate the ord...   \n",
       "16                        What datasets did they use?   \n",
       "17  How is attention computed efficiently in pract...   \n",
       "18  How did the Transformer (big) model perform on...   \n",
       "19  What BLEU score did the Transformer (big) mode...   \n",
       "\n",
       "                                             Contexto  \\\n",
       "0   [2.3 Ejemplo RNN\\n\\nEstas redes se suelen util...   \n",
       "1   [1.1 Problemas de las RNN est´andar\\n\\nEstas r...   \n",
       "2   [3.3 LSTM\\n\\nImagen de flujo de informaci´on e...   \n",
       "3   [3.2 Componentes clave de las LSTM\\n\\nLos comp...   \n",
       "4   [4.3 GRU\\n\\nLas Gated Recurrent Units (GRU) so...   \n",
       "5   [4.3 GRU\\n\\nLas Gated Recurrent Units (GRU) so...   \n",
       "6   [6 Limitaciones y mejores pr´acticas de LSTM\\n...   \n",
       "7   [6 Limitaciones y mejores pr´acticas de LSTM\\n...   \n",
       "8   [6 Limitaciones y mejores pr´acticas de LSTM\\n...   \n",
       "9   [6 Limitaciones y mejores pr´acticas de LSTM\\n...   \n",
       "10  [3.2 Attention\\n\\nAn attention function can be...   \n",
       "11  [3.2.2 Multi-Head Attention\\n\\nInstead of perf...   \n",
       "12  [3.2.2 Multi-Head Attention\\n\\nInstead of perf...   \n",
       "13  [4 Why Self-Attention\\n\\nIn this section we co...   \n",
       "14  [3.3 Position-wise Feed-Forward Networks\\n\\nIn...   \n",
       "15  [1 Introduction\\n\\nRecurrent neural networks, ...   \n",
       "16  [5 Training\\n\\nThis section describes the trai...   \n",
       "17  [3.2 Attention\\n\\nAn attention function can be...   \n",
       "18  [6 Results\\n\\n6.1 Machine Translation\\n\\nOn th...   \n",
       "19  [6 Results\\n\\n6.1 Machine Translation\\n\\nOn th...   \n",
       "\n",
       "                                     Respuesta_Modelo  \\\n",
       "0   Una RNN, o red neuronal recurrente, es un tipo...   \n",
       "1   Las RNNs (Redes Neuronales Recurrentes) enfren...   \n",
       "2   Las LSTM (Long Short-Term Memory) son un tipo ...   \n",
       "3   Las componentes clave de las LSTM son:\\n\\nCélu...   \n",
       "4   Las Gated Recurrent Units (GRU) son un tipo de...   \n",
       "5   Las LSTM (Long Short-Term Memory) y las GRU (G...   \n",
       "6   Las limitaciones de las LSTM son:\\n\\nComplejid...   \n",
       "7   Respecto al ajuste de hiperparámetros en LSTM,...   \n",
       "8   Hay varias técnicas para reducir el sobreajust...   \n",
       "9   Los casos de uso de las LSTM incluyen:\\n\\nTrad...   \n",
       "10  An attention function is a mechanism that maps...   \n",
       "11  The Transformer model uses attention in three ...   \n",
       "12  Multi-Head Attention is an attention mechanism...   \n",
       "13  Self-attention is favored for several reasons ...   \n",
       "14  Each layer in the encoder and decoder of the T...   \n",
       "15  The model incorporates the order of the sequen...   \n",
       "16  The training datasets used in the research are...   \n",
       "17  Attention is computed efficiently in practice ...   \n",
       "18  The Transformer (big) model achieved a BLEU sc...   \n",
       "19  The Transformer (big) model achieved a BLEU sc...   \n",
       "\n",
       "                                 Respuesta_Referencia  faithfulness  \\\n",
       "0   Las Redes Neuronales Recurrentes son un tipo e...      0.814973   \n",
       "1   Estas redes se enfrentan a un problema fundame...      0.768091   \n",
       "2   Las LSTMs (Long Short-Term Memory) son un tipo...      0.797648   \n",
       "3   Los componentes clave de las LSTM que las dist...      0.702831   \n",
       "4   Las Gated Recurrent Units (GRU) son un tipo de...      0.794439   \n",
       "5   Comparación entre LSTM y GRU\\n• Complejidad: E...      0.866765   \n",
       "6   Limitaciones de las LSTM\\n• Complejidad y cost...      0.749903   \n",
       "7   Ajuste de hiperparámetros:\\n- Tamaño del númer...      0.669974   \n",
       "8   Uso de regulación para evitar sobreajuste:\\n- ...      0.606070   \n",
       "9   Casos de uso de las LSTM\\n• Dependencias tempo...      0.682564   \n",
       "10  An attention function can be described as mapp...      0.583461   \n",
       "11  The Transformer uses multi-head attention in t...      0.624016   \n",
       "12  Instead of performing a single attention funct...      0.653579   \n",
       "13  In this section we compare various aspects of ...      0.618472   \n",
       "14  In addition to attention sub-layers, each of t...      0.720588   \n",
       "15  Since our model contains no recurrence and no ...      0.674769   \n",
       "16  We trained on the standard WMT 2014 English-Ge...      0.605352   \n",
       "17  In practice, we compute the attention function...      0.711568   \n",
       "18  The Transformer (big) model achieved a BLEU sc...      0.655258   \n",
       "19  It achieved a BLEU score of 41.0 on the WMT 20...      0.605911   \n",
       "\n",
       "    answer_relevancy  context_precision  answer_similarity  BERTScore_F1  \n",
       "0           0.777145           1.000000           0.815128      0.710676  \n",
       "1           0.764773           1.000000           0.903567      0.777857  \n",
       "2           0.815644           1.000000           0.938347      0.831891  \n",
       "3           0.794647           1.000000           0.931384      0.758998  \n",
       "4           0.683109           0.000000           0.974084      0.901622  \n",
       "5           0.790532           1.000000           0.956360      0.797000  \n",
       "6           0.819090           1.000000           0.958100      0.873939  \n",
       "7           0.839722           0.666667           0.890632      0.817970  \n",
       "8           0.839068           0.333333           0.893565      0.820560  \n",
       "9           0.716270           0.333333           0.872708      0.733046  \n",
       "10          0.727181           0.000000           0.934127      0.923933  \n",
       "11          0.516775           0.000000           0.898642      0.894265  \n",
       "12          0.765651           0.500000           0.878023      0.911197  \n",
       "13          0.609084           0.000000           0.842773      0.859169  \n",
       "14          0.698126           0.500000           0.850841      0.900421  \n",
       "15          0.638088           0.000000           0.784113      0.870603  \n",
       "16          0.512609           0.000000           0.789078      0.859309  \n",
       "17          0.670940           0.000000           0.844505      0.840961  \n",
       "18          0.792248           0.666667           0.821527      0.918985  \n",
       "19          0.853187           0.500000           0.723984      0.898398  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concatenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856a6e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_concatenado.to_excel('tabla_comparativa.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
